"""
Amazon å•†å“è¯¦æƒ…é¡µçˆ¬è™«
åŸºäº DrissionPage åº“å®ç°
æ ¹æ® ASIN ç¼–ç çˆ¬å–å•†å“è¯¦æƒ…ä¿¡æ¯ï¼šæ ‡é¢˜ã€äº”ç‚¹æè¿°ã€ä»·æ ¼ã€å±æ€§ã€A+é¡µå›¾ç‰‡
"""
import time
import os
import json
import re
from typing import List, Dict, Optional
from DrissionPage import ChromiumPage, ChromiumOptions, Chromium
import requests
from pathlib import Path


class AmazonDetailCrawler:
    """Amazon å•†å“è¯¦æƒ…é¡µçˆ¬è™«ç±»"""
    
    def __init__(self, headless: bool = False, use_saved_login: bool = True, local_port: int = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            use_saved_login: æ˜¯å¦ä½¿ç”¨ä¿å­˜çš„ç™»å½•ä¿¡æ¯ï¼ˆç”¨æˆ·æ•°æ®ç›®å½•ï¼‰
            local_port: æ¥ç®¡æœ¬åœ°æµè§ˆå™¨çš„ç«¯å£å·ï¼ˆå¦‚ 9333ï¼‰ï¼Œå¦‚æœæŒ‡å®šåˆ™å¿½ç•¥å…¶ä»–å‚æ•°
        """
        self.page = None
        self.browser = None
        self.headless = headless
        self.use_saved_login = use_saved_login
        self.local_port = local_port
        self.base_url = "https://www.amazon.com/dp/"
        self._init_browser()

    def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨é…ç½®ï¼Œè®¾ç½®å»æŒ‡çº¹å‚æ•°"""
        # å¦‚æœæŒ‡å®šäº†æœ¬åœ°ç«¯å£ï¼Œç›´æ¥æ¥ç®¡å·²æ‰“å¼€çš„æµè§ˆå™¨
        if self.local_port:
            print(f"æ­£åœ¨æ¥ç®¡æœ¬åœ°æµè§ˆå™¨ï¼ˆç«¯å£: {self.local_port}ï¼‰...")
            print(f"æç¤º: è¯·ç¡®ä¿å·²ç”¨ --remote-debugging-port={self.local_port} å¯åŠ¨ Microsoft Edge")

            try:
                # åˆ›å»ºé…ç½®
                from DrissionPage import ChromiumOptions
                co = ChromiumOptions()

                # ========== æŒ‡å®š Microsoft Edge è·¯å¾„ ==========
                # Windows 10/11 æ ‡å‡† Edge è·¯å¾„
                edge_path = r'C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe'

                # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
                if os.path.exists(edge_path):
                    co.set_browser_path(edge_path)
                    print(f"âœ… æŒ‡å®š Edge è·¯å¾„: {edge_path}")
                else:
                    # å°è¯•å…¶ä»–å¯èƒ½è·¯å¾„
                    alt_paths = [
                        r'C:\Program Files\Microsoft\Edge\Application\msedge.exe',  # ARM64ç‰ˆæœ¬
                        r'C:\Users\{}\AppData\Local\Microsoft\Edge\Application\msedge.exe'.format(os.getlogin()),
                        # ç”¨æˆ·å®‰è£…
                    ]

                    for path in alt_paths:
                        if os.path.exists(path):
                            co.set_browser_path(path)
                            print(f"âœ… æ‰¾åˆ° Edge å¤‡ç”¨è·¯å¾„: {path}")
                            break
                    else:
                        print("âš ï¸ æœªæ‰¾åˆ° Edge è·¯å¾„ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤")

                # è®¾ç½®æ¥ç®¡ç«¯å£
                co.set_local_port(self.local_port)

                # éæ— å¤´æ¨¡å¼
                if not self.headless:
                    co.headless(False)

                # Edge ç‰¹å®šé…ç½®
                co.set_argument('--disable-features=EdgeTranslate')
                co.set_argument('--disable-component-update')

                # åˆ›å»ºé¡µé¢å¯¹è±¡ï¼ˆæ¥ç®¡æ¨¡å¼ï¼‰
                self.page = ChromiumPage(co)
                print("âœ… å·²æˆåŠŸæ¥ç®¡ Microsoft Edge æµè§ˆå™¨")
                return

            except Exception as e:
                print(f"âŒ æ¥ç®¡å¤±è´¥: {e}")
                print("\nğŸ’¡ Edge æµè§ˆå™¨å¯åŠ¨æ­¥éª¤:")
                print("1. å…³é—­æ‰€æœ‰ Edge çª—å£")
                print("2. è¿è¡Œå¯åŠ¨å‘½ä»¤:")
                print('   msedge.exe --remote-debugging-port=9333 --remote-allow-origins=*')
                print("3. æˆ–è¿è¡Œè„šæœ¬: ..\\start_edge.bat")
                print("4. ä¿æŒ Edge çª—å£æ‰“å¼€ï¼Œç„¶åé‡è¯•")
                raise

        # ========== è‡ªåŠ¨å¯åŠ¨æ¨¡å¼ï¼ˆå½“æœªæŒ‡å®šlocal_portæ—¶ï¼‰ ==========
        print("è‡ªåŠ¨å¯åŠ¨ Microsoft Edge æµè§ˆå™¨...")

        # é…ç½®æµè§ˆå™¨é€‰é¡¹
        co = ChromiumOptions()

        # ========== è®¾ç½® Microsoft Edge ä¸ºé»˜è®¤æµè§ˆå™¨ ==========
        # Windows Edge æ ‡å‡†è·¯å¾„
        default_edge_paths = [
            r'C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe',  # 32ä½æ ‡å‡†ç‰ˆ
            r'C:\Program Files\Microsoft\Edge\Application\msedge.exe',  # 64ä½ç‰ˆ
            r'C:\Users\{}\AppData\Local\Microsoft\Edge\Application\msedge.exe'.format(os.getlogin()),  # ç”¨æˆ·å®‰è£…
        ]

        edge_found = False
        for path in default_edge_paths:
            if os.path.exists(path):
                co.set_browser_path(path)
                print(f"âœ… ä½¿ç”¨ Microsoft Edge: {path}")
                edge_found = True
                break

        if not edge_found:
            print("âš ï¸ æœªæ‰¾åˆ° Microsoft Edgeï¼Œå°†ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æµè§ˆå™¨")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£… Microsoft Edge æµè§ˆå™¨")

        # Edge ä¼˜åŒ–é…ç½®
        co.set_argument('--disable-blink-features=AutomationControlled')
        co.set_argument('--disable-gpu')
        co.set_argument('--no-sandbox')
        co.set_argument('--disable-dev-shm-usage')

        # Edge ç‰¹å®šé…ç½®
        co.set_argument('--disable-features=EdgeTranslate,EdgeCollections')
        co.set_argument('--disable-component-update')
        co.set_argument('--lang=zh-CN')  # è®¾ç½®ä¸­æ–‡è¯­è¨€

        # è®¾ç½® User-Agent (Edge)
        co.set_user_agent(
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        )

        # æ˜¯å¦ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•ï¼ˆä¿å­˜ç™»å½•ä¿¡æ¯ï¼‰
        if self.use_saved_login:
            user_data_dir = os.path.join(os.path.dirname(__file__), 'edge_browser_data')
            co.set_user_data_path(user_data_dir)
            print(f"âœ… ä½¿ç”¨ Edge ç”¨æˆ·æ•°æ®ç›®å½•: {user_data_dir}")

        # æ— å¤´æ¨¡å¼
        if self.headless:
            co.headless()
        else:
            co.headless(False)
            co.set_argument('--start-maximized')  # å¯åŠ¨æ—¶æœ€å¤§åŒ–

        try:
            # åˆå§‹åŒ–é¡µé¢
            self.page = ChromiumPage(addr_or_opts=co)

            # æ‰§è¡Œ JavaScript å»é™¤ webdriver ç‰¹å¾
            self.page.run_js('''
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            ''')

            print(f"âœ… Microsoft Edge æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            print(f"ğŸŒ æµè§ˆå™¨ç‰ˆæœ¬: {self.page.browser.version if hasattr(self.page, 'browser') else 'æœªçŸ¥'}")

        except Exception as e:
            print(f"âŒ Edge æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")

            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç®€åŒ–é…ç½®
            print("ğŸ”„ å°è¯•ç®€åŒ–é…ç½®å¯åŠ¨...")
            try:
                co_simple = ChromiumOptions()
                co_simple.headless(False)
                self.page = ChromiumPage(co_simple)
                print("âœ… ç®€åŒ–é…ç½®å¯åŠ¨æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ ç®€åŒ–é…ç½®ä¹Ÿå¤±è´¥: {e2}")
                raise
    
    def crawl_product(self, asin: str) -> Optional[Dict]:
        """
        çˆ¬å–å•ä¸ªå•†å“è¯¦æƒ…
        
        Args:
            asin: å•†å“ ASIN ç¼–ç 
            
        Returns:
            å•†å“è¯¦æƒ…æ•°æ®å­—å…¸
        """
        product_data = {
            'asin': asin,
            'url': f"{self.base_url}{asin}",
            'title': None,
            'bullet_points': [],
            'price': None,
            'product_details': {},  # ç°åœ¨æ˜¯åµŒå¥—å­—å…¸: {"Table Name": {"key": "value"}}
            'aplus_images': []
        }
        
        try:
            print(f"\næ­£åœ¨çˆ¬å– ASIN: {asin}")
            url = f"{self.base_url}{asin}"
            self.page.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # æå–æ ‡é¢˜
            product_data['title'] = self._extract_title()
            
            # æå–äº”ç‚¹æè¿°
            product_data['bullet_points'] = self._extract_bullet_points()
            
            # æå–ä»·æ ¼
            product_data['price'] = self._extract_price()
            
            # æå–å•†å“è¯¦æƒ…
            product_data['product_details'] = self._extract_product_details()
            
            # æå– A+ é¡µå›¾ç‰‡
            product_data['aplus_images'] = self._extract_aplus_images(asin)
            
            print(f"âœ… æˆåŠŸçˆ¬å– ASIN: {asin}")
            return product_data
            
        except Exception as e:
            print(f"âŒ çˆ¬å– ASIN {asin} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return product_data
    
    def _extract_title(self) -> Optional[str]:
        """æå–å•†å“æ ‡é¢˜"""
        try:
            title_element = self.page.ele('xpath://span[@id="productTitle"]', timeout=5)
            if title_element:
                title = title_element.text.strip()
                print(f"  æ ‡é¢˜: {title[:50]}...")
                return title
        except Exception as e:
            print(f"  âš ï¸ æå–æ ‡é¢˜å¤±è´¥: {e}")
        return None
    
    def _extract_bullet_points(self) -> List[str]:
        """æå–äº”ç‚¹æè¿°"""
        bullet_points = []
        try:
            bullets_container = self.page.ele('xpath://div[@id="feature-bullets"]', timeout=5)
            if bullets_container:
                # æŸ¥æ‰¾æ‰€æœ‰ li å…ƒç´ 
                li_elements = bullets_container.eles('tag:li')
                for li in li_elements:
                    text = li.text.strip()
                    # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬å’Œ"See more product details"ç­‰æ— å…³å†…å®¹
                    if text and not text.startswith('See more') and len(text) > 10:
                        bullet_points.append(text)
                print(f"  äº”ç‚¹æè¿°: å…± {len(bullet_points)} æ¡")
        except Exception as e:
            print(f"  âš ï¸ æå–äº”ç‚¹æè¿°å¤±è´¥: {e}")
        return bullet_points
    
    def _extract_price(self) -> Optional[str]:
        """æå–ä»·æ ¼ï¼ˆå°è¯•å¤šä¸ªXPathï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¸¦è´§å¸ç¬¦å·çš„ä»·æ ¼ï¼‰"""
        price_xpaths = [
            '//div[@id="apex_desktop_newAccordionRow"]//div[@id="corePriceDisplay_desktop_feature_div"]//span[@aria-hidden="true"]',
            '//span[@class="a-price aok-align-center reinventPricePriceToPayMargin priceToPay"]//span[@aria-hidden="true"]',
            '//span[@class="a-price-whole"]',
            '//span[contains(@class, "a-price")]//span[@class="a-offscreen"]',
            '//span[contains(@class, "a-price")]//span[@aria-hidden="true"]'
        ]
        
        # è´§å¸ç¬¦å·åˆ—è¡¨
        currency_symbols = ['$', 'Â¥', 'â‚¬', 'Â£', 'â‚¹', 'â‚½', 'â‚©', 'Â¢', 'R$', 'CA$', 'AU$', 'HK$', 'NZ$', 'S$']
        
        for xpath in price_xpaths:
            try:
                # å°è¯•è·å–æ‰€æœ‰åŒ¹é…çš„å…ƒç´ 
                price_elements = self.page.eles(f'xpath:{xpath}', timeout=2)
                if not price_elements:
                    continue
                
                # éå†æ‰€æœ‰å…ƒç´ ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å«è´§å¸ç¬¦å·çš„ä»·æ ¼
                for price_element in price_elements:
                    price_text = price_element.text.strip()
                    if not price_text:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è´§å¸ç¬¦å·
                    has_currency = any(symbol in price_text for symbol in currency_symbols)
                    
                    # æ’é™¤æŠ˜æ‰£ç™¾åˆ†æ¯”ï¼ˆå¦‚ -28%ï¼‰å’Œçº¯æ–‡æœ¬ï¼ˆå¦‚ List Price:ï¼‰
                    is_discount = '%' in price_text and '-' in price_text
                    is_label = any(label in price_text.lower() for label in ['list price', 'was:', 'save', 'typical'])
                    
                    # å¦‚æœåŒ…å«è´§å¸ç¬¦å·ä¸”ä¸æ˜¯æŠ˜æ‰£æˆ–æ ‡ç­¾ï¼Œè¿”å›è¯¥ä»·æ ¼
                    if has_currency and not is_discount and not is_label:
                        print(f"  ä»·æ ¼: {price_text}")
                        return price_text
                
            except Exception as e:
                continue
        
        print("  âš ï¸ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
        return None
    
    def _extract_product_details(self) -> Dict[str, Dict[str, str]]:
        """æå–å•†å“è¯¦æƒ…å±æ€§ï¼ˆæŒ‰è¡¨æ ¼åˆ†ç»„ï¼‰"""
        details = {}
        total_items = 0
        
        try:
            # æ–¹æ³•1: ä¼˜å…ˆä»å·¦å³ä¸¤ä¾§çš„è¯¦æƒ…è¡¨æ ¼æå–ï¼ˆå™ªéŸ³æ›´å°‘ï¼‰
            left_sections = self.page.ele('xpath://div[@id="productDetails_expanderTables_depthLeftSections"]', timeout=5)
            right_sections = self.page.ele('xpath://div[@id="productDetails_expanderTables_depthRightSections"]', timeout=5)
            
            all_sections = []
            if left_sections:
                all_sections.append(('Left', left_sections))
            if right_sections:
                all_sections.append(('Right', right_sections))
            
            if all_sections:
                print(f"  æ‰¾åˆ°å·¦å³è¯¦æƒ…åŒºåŸŸ: {len(all_sections)} ä¸ª")
                
                for section_name, section_container in all_sections:
                    # åœ¨æ¯ä¸ªå®¹å™¨ä¸­æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼åˆ†ç»„
                    expander_divs = section_container.eles('xpath:.//div[contains(@class, "a-expander-container")]')
                    
                    print(f"    [{section_name}] æ‰¾åˆ° {len(expander_divs)} ä¸ªè¡¨æ ¼åˆ†ç»„")
                    
                    for expander in expander_divs:
                        try:
                            # æå–æ ‡é¢˜
                            title_elem = expander.ele('xpath:.//span[@class="a-expander-prompt"]')
                            section_title = title_elem.text.strip() if title_elem else None
                            
                            if not section_title:
                                continue
                            
                            # æå–è¡¨æ ¼
                            table = expander.ele('xpath:.//table[contains(@class, "prodDetTable")]')
                            if not table:
                                continue
                            
                            section_data = {}
                            rows = table.eles('tag:tr')
                            
                            for row in rows:
                                try:
                                    th = row.ele('tag:th')
                                    td = row.ele('tag:td')
                                    
                                    if th and td:
                                        key = th.text.strip()
                                        value = td.text.strip()
                                        
                                        # è¿‡æ»¤æ‰å™ªéŸ³æ•°æ®ï¼ˆå¦‚è„šæœ¬ã€è¯„è®ºç­‰ï¼‰
                                        if key and value and len(key) < 100 and not key.startswith('var '):
                                            key = ' '.join(key.split())
                                            value = ' '.join(value.split())
                                            section_data[key] = value
                                            total_items += 1
                                except:
                                    continue
                            
                            if section_data:
                                final_title = section_title
                                counter = 2
                                while final_title in details:
                                    final_title = f"{section_title} {counter}"
                                    counter += 1
                                
                                details[final_title] = section_data
                                print(f"      âœ… [{final_title}]: {len(section_data)} é¡¹")
                                
                        except Exception as e:
                            continue
            
            # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œé™çº§åˆ°ä» prodDetails ä¸‹çš„å¯æŠ˜å è¡¨æ ¼æå–
            if not details:
                print("  âš ï¸ æœªæ‰¾åˆ°å·¦å³è¯¦æƒ…åŒºåŸŸï¼Œå°è¯•ä» prodDetails æå–...")
                
                prod_details = self.page.ele('xpath://div[@id="prodDetails"]', timeout=3)
                if prod_details:
                    expander_sections = prod_details.eles('xpath:.//div[contains(@class, "a-expander-container")]')
                    
                    if expander_sections:
                        print(f"  æ‰¾åˆ° {len(expander_sections)} ä¸ªå¯æŠ˜å è¡¨æ ¼")
                        
                        for section in expander_sections:
                            try:
                                # æå–è¡¨æ ¼æ ‡é¢˜
                                title_elem = section.ele('xpath:.//span[@class="a-expander-prompt"]')
                                section_title = title_elem.text.strip() if title_elem else None
                                
                                if not section_title:
                                    section_title = "Unknown Section"
                                
                                # æå–è¡¨æ ¼æ•°æ®
                                table = section.ele('xpath:.//table[contains(@class, "prodDetTable")]')
                                if not table:
                                    continue
                                
                                section_data = {}
                                rows = table.eles('tag:tr')
                                
                                for row in rows:
                                    try:
                                        th = row.ele('tag:th')
                                        td = row.ele('tag:td')
                                        
                                        if th and td:
                                            key = th.text.strip()
                                            value = td.text.strip()
                                            
                                            if key and value:
                                                key = ' '.join(key.split())
                                                value = ' '.join(value.split())
                                                section_data[key] = value
                                                total_items += 1
                                    except:
                                        continue
                                
                                if section_data:
                                    final_title = section_title
                                    counter = 2
                                    while final_title in details:
                                        final_title = f"{section_title} {counter}"
                                        counter += 1
                                    
                                    details[final_title] = section_data
                                    print(f"    âœ… [{final_title}]: {len(section_data)} é¡¹")
                                    
                            except Exception as e:
                                print(f"    âš ï¸ å¤„ç†è¡¨æ ¼åˆ†ç»„å¤±è´¥: {e}")
                                continue
            
            print(f"  å•†å“å±æ€§: å…± {len(details)} ä¸ªè¡¨æ ¼, {total_items} é¡¹")
            
        except Exception as e:
            print(f"  âš ï¸ æå–å•†å“è¯¦æƒ…å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return details
    
    def _extract_aplus_images(self, asin: str) -> List[Dict[str, str]]:
        """
        æå– A+ é¡µé¢çš„å›¾ç‰‡
        
        Returns:
            å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« url å’Œ local_path
        """
        images = []
        
        # ä¸´æ—¶å…³é—­å›¾ç‰‡ä¸‹è½½åŠŸèƒ½ä»¥æé«˜çˆ¬å–é€Ÿåº¦
        print("  â­ï¸  å·²è·³è¿‡ A+ å›¾ç‰‡ä¸‹è½½ï¼ˆåŠŸèƒ½å·²å…³é—­ï¼‰")
        return images
        
        seen_urls = set()  # ç”¨äºå»é‡
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« aplus çš„å®¹å™¨
            aplus_containers = self.page.eles('xpath://div[contains(@class, "aplus")]')

            print(f"  æ‰¾åˆ° {len(aplus_containers)} ä¸ª A+ å®¹å™¨")

            for idx, container in enumerate(aplus_containers):
                # åœ¨æ¯ä¸ªå®¹å™¨ä¸­æŸ¥æ‰¾å›¾ç‰‡
                imgs = container.eles('tag:img')
                for img in imgs:
                    img_url = img.attr('src') or img.attr('data-src')
                    if img_url and 'aplus-media' in img_url:
                        # ç¡®ä¿ä½¿ç”¨é«˜è´¨é‡å›¾ç‰‡é“¾æ¥
                        if img_url.startswith('//'):
                            img_url = 'https:' + img_url

                        # å»é‡ï¼šæ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                        if img_url in seen_urls:
                            continue
                        seen_urls.add(img_url)

                        images.append({
                            'url': img_url,
                            'container_index': idx,
                            'local_path': None,  # ç¨åä¸‹è½½æ—¶å¡«å……
                            'file_size': None  # ç¨åä¸‹è½½æ—¶å¡«å……
                        })

            print(f"  A+ å›¾ç‰‡ï¼ˆå»é‡å‰ï¼‰: {len(aplus_containers)} ä¸ªå®¹å™¨")
            print(f"  A+ å›¾ç‰‡ï¼ˆå»é‡åï¼‰: {len(images)} å¼ ")

            # ä¸‹è½½å›¾ç‰‡
            if images:
                downloaded = self._download_aplus_images(images, asin)
                # åªä¿ç•™æˆåŠŸä¸‹è½½çš„å›¾ç‰‡
                images = [img for img in images if img['local_path']]
                print(f"  A+ å›¾ç‰‡ï¼ˆæœ€ç»ˆä¿å­˜ï¼‰: {len(images)} å¼ ")

        except Exception as e:
            print(f"  âš ï¸ æå– A+ å›¾ç‰‡å¤±è´¥: {e}")

        return images
    
    def _download_aplus_images(self, images: List[Dict], asin: str, min_size_kb: int = 100):
        """
        ä¸‹è½½ A+ å›¾ç‰‡åˆ°æœ¬åœ°ï¼ˆè¿‡æ»¤å°äºæŒ‡å®šå¤§å°çš„å›¾ç‰‡ï¼‰
        
        Args:
            images: å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨
            asin: å•†å“ ASIN
            min_size_kb: æœ€å°æ–‡ä»¶å¤§å°ï¼ˆKBï¼‰ï¼Œé»˜è®¤100KB
        """
        # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
        img_dir = Path(__file__).parent / 'aplus_images' / asin
        img_dir.mkdir(parents=True, exist_ok=True)
        
        downloaded_count = 0
        skipped_small = 0
        
        for idx, img_info in enumerate(images):
            try:
                url = img_info['url']
                
                # å…ˆç”¨HEADè¯·æ±‚è·å–æ–‡ä»¶å¤§å°ï¼Œé¿å…ä¸‹è½½å°å›¾ç‰‡
                try:
                    head_response = requests.head(url, timeout=5, allow_redirects=True)
                    content_length = head_response.headers.get('Content-Length')
                    
                    if content_length:
                        file_size_bytes = int(content_length)
                        file_size_kb = file_size_bytes / 1024
                        
                        # è¿‡æ»¤å°äºæŒ‡å®šå¤§å°çš„å›¾ç‰‡
                        if file_size_kb < min_size_kb:
                            skipped_small += 1
                            print(f"    â­ï¸  è·³è¿‡å°å›¾ç‰‡ {idx+1}/{len(images)}: {file_size_kb:.1f}KB < {min_size_kb}KB")
                            continue
                except:
                    # å¦‚æœHEADè¯·æ±‚å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹è½½
                    pass
                
                # ä¸‹è½½å›¾ç‰‡
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    content = response.content
                    file_size_bytes = len(content)
                    file_size_kb = file_size_bytes / 1024
                    
                    # å†æ¬¡æ£€æŸ¥å®é™…ä¸‹è½½çš„æ–‡ä»¶å¤§å°
                    if file_size_kb < min_size_kb:
                        skipped_small += 1
                        print(f"    â­ï¸  è·³è¿‡å°å›¾ç‰‡ {idx+1}/{len(images)}: {file_size_kb:.1f}KB < {min_size_kb}KB")
                        continue
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    ext = '.jpg'
                    filename = f"aplus_{downloaded_count+1}_{int(file_size_kb)}kb{ext}"
                    filepath = img_dir / filename
                    
                    # ä¿å­˜å›¾ç‰‡
                    with open(filepath, 'wb') as f:
                        f.write(content)
                    
                    img_info['local_path'] = str(filepath)
                    img_info['file_size'] = f"{file_size_kb:.1f}KB"
                    downloaded_count += 1
                    print(f"    âœ… ä¸‹è½½å›¾ç‰‡ {downloaded_count}: {filename} ({file_size_kb:.1f}KB)")
                else:
                    print(f"    âš ï¸ ä¸‹è½½å¤±è´¥ {idx+1}/{len(images)}: HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"    âš ï¸ ä¸‹è½½å›¾ç‰‡ {idx+1} å¤±è´¥: {e}")
        
        print(f"\n  ğŸ“Š ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {downloaded_count} å¼ , è·³è¿‡å°å›¾ç‰‡ {skipped_small} å¼ ")
        return downloaded_count
    
    def crawl_products_from_list(self, asins: List[str], output_file: str = 'amazon_products.json'):
        """
        æ‰¹é‡çˆ¬å–å•†å“åˆ—è¡¨
        
        Args:
            asins: ASIN åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        all_products = []
        
        for idx, asin in enumerate(asins, 1):
            print(f"\n{'='*60}")
            print(f"è¿›åº¦: {idx}/{len(asins)}")
            print(f"{'='*60}")
            
            product_data = self.crawl_product(asin)
            if product_data:
                all_products.append(product_data)
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if idx < len(asins):
                time.sleep(2)
        
        # ä¿å­˜ç»“æœ
        self._save_results(all_products, output_file)
        
        return all_products
    
    def _save_results(self, products: List[Dict], output_file: str):
        """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
        try:
            output_path = Path(__file__).parent / output_file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            try:
                self.page.close()
                print("æ ‡ç­¾é¡µå·²å…³é—­")
            except:
                print("æ— éœ€å…³é—­æ ‡ç­¾é¡µï¼ˆæ¥ç®¡æ¨¡å¼ï¼‰")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    print("="*60)
    print("Amazon å•†å“è¯¦æƒ…çˆ¬è™«")
    print("="*60)
    
    # æµ‹è¯• ASIN åˆ—è¡¨
    test_asins = [
        'B0DQKSVC1B',  # ç¤ºä¾‹ ASIN
        # å¯ä»¥æ·»åŠ æ›´å¤š ASIN
    ]
    
    # ä½¿ç”¨æ¥ç®¡æ¨¡å¼ï¼šè¿æ¥åˆ°å·²æ‰“å¼€çš„æµè§ˆå™¨ï¼ˆç«¯å£ 9333ï¼‰
    print("\nã€æ¥ç®¡æ¨¡å¼ã€‘è¿æ¥åˆ°å·²æ‰“å¼€çš„æµè§ˆå™¨ï¼ˆç«¯å£ 9333ï¼‰...")
    print("æç¤º: è¯·å…ˆè¿è¡Œå¯åŠ¨Chromeè°ƒè¯•æ¨¡å¼.bat")
    print()
    
    crawler = AmazonDetailCrawler(local_port=9333)
    
    try:
        # æ‰¹é‡çˆ¬å–
        products = crawler.crawl_products_from_list(
            asins=test_asins,
            output_file='amazon_products.json'
        )
        
        print("\n" + "="*60)
        print("çˆ¬å–å®Œæˆ")
        print("="*60)
        print(f"æˆåŠŸçˆ¬å– {len(products)} ä¸ªå•†å“")
        
    finally:
        crawler.close()
