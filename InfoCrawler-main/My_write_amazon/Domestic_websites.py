import re
import time
import json
import csv
import random
from typing import List, Dict, Optional
from urllib.parse import urljoin, quote
import os
from DrissionPage import ChromiumPage, ChromiumOptions
from datetime import datetime


class ChineseEcommerceDetailCrawler:
    """å›½å†…ç”µå•†å¹³å°å•†å“è¯¦æƒ…çˆ¬è™«ï¼ˆç›´æ¥çˆ¬å–å•†å“é¡µï¼‰"""

    def __init__(self, headless: bool = False, use_saved_login: bool = True):
        """
        åˆå§‹åŒ–çˆ¬è™«
        """
        self.page = None
        self.headless = headless
        self.use_saved_login = use_saved_login
        self.current_site = 'unknown'

        # å„ç«™ç‚¹å•†å“è¯¦æƒ…é¡µé…ç½®
        self.site_detail_configs = {
            'taobao': {
                'name': 'æ·˜å®',
                'search_url': 'https://s.taobao.com/search?q={keyword}',
                'item_url_patterns': [
                    r'https://item\.taobao\.com/item\.htm\?id=\d+',
                    r'//item\.taobao\.com/item\.htm\?id=\d+'
                ],
                # å•†å“è¯¦æƒ…é¡µé€‰æ‹©å™¨
                'detail_selectors': {
                    'title': ['h1[class*="Title"]', 'div.tb-detail-hd h1', '#J_Title'],
                    'price': ['.tb-rmb-num', '.tm-price', '#J_StrPrice'],
                    'original_price': ['.tm-originalPrice', '.tb-rmb-line'],
                    'sales': ['.tm-count', '#J_SellCounter'],
                    'shop_name': ['.tb-shop-name', '.shop-name'],
                    'shop_link': ['.tb-shop-name a', '.shop-name a'],
                    'description': ['#description', '.tb-detail-content'],
                    'specs': ['.tb-key .tb-prop'],
                    'images': ['.tb-booth img', '#J_UlThumb img'],
                    'rating': ['.tb-rate-counter'],
                    'comments_count': ['.tm-ind-panel .tm-count'],
                    'stock': ['.tb-amount'],
                    'sku': ['.tb-sku'],
                    'coupon': ['.tb-coupon'],
                    'promotion': ['.tb-promotion']
                }
            },
            'jd': {
                'name': 'äº¬ä¸œ',
                'search_url': 'https://search.jd.com/Search?keyword={keyword}&enc=utf-8',
                'item_url_patterns': [
                    r'https://item\.jd\.com/\d+\.html',
                    r'//item\.jd\.com/\d+\.html'
                ],
                'detail_selectors': {
                    'title': ['.sku-name', 'div[class*="name"]'],
                    'price': ['.p-price .price', '.J-p-{}'],  # {} ä¼šè¢«å•†å“IDæ›¿æ¢
                    'original_price': ['.p-price del'],
                    'sales': ['.p-sales', '#sales', '.count'],
                    'shop_name': ['.J-hover-wrap .name', '.shop-name'],
                    'shop_link': ['.J-hover-wrap a', '.shop-name a'],
                    'description': ['.detail-content', '#product-detail'],
                    'specs': ['.p-parameter-list', '#parameter2'],
                    'images': ['.spec-items img', '#spec-list img'],
                    'rating': ['.percent-con'],
                    'comments_count': ['.comment-count'],
                    'stock': ['.store-prompt', '.stock'],
                    'sku': ['.itemInfo-wrap'],
                    'coupon': ['.coupon'],
                    'promotion': ['.prom-goods']
                }
            },
            'tmall': {
                'name': 'å¤©çŒ«',
                'search_url': 'https://list.tmall.com/search_product.htm?q={keyword}',
                'item_url_patterns': [
                    r'https://detail\.tmall\.com/item\.htm\?id=\d+',
                    r'//detail\.tmall\.com/item\.htm\?id=\d+'
                ],
                'detail_selectors': {
                    'title': ['.tb-detail-hd h1', '.tb-main-title'],
                    'price': ['.tm-price', '.tm-price-panel'],
                    'original_price': ['.tm-originalPrice'],
                    'sales': ['.tm-count'],
                    'shop_name': ['.tb-shop-name', '.slogo-shopname'],
                    'shop_link': ['.tb-shop-name a'],
                    'description': ['.tb-detail-content', '#J_DivItemDesc'],
                    'specs': ['.tb-key'],
                    'images': ['.tb-booth img'],
                    'rating': ['.tb-rate-counter'],
                    'comments_count': ['.tm-review'],
                    'stock': ['.tb-amount'],
                    'sku': ['.tb-sku'],
                    'coupon': ['.tb-coupon'],
                    'promotion': ['.tb-promotion']
                }
            },
            '1688': {
                'name': '1688',
                'search_url': 'https://s.1688.com/selloffer/offer_search.html?keywords={keyword}',
                'item_url_patterns': [
                    r'https://detail\.1688\.com/offer/\d+\.html',
                    r'//detail\.1688\.com/offer/\d+\.html'
                ],
                'detail_selectors': {
                    'title': ['.offer-title', '.title'],
                    'price': ['.price', '.offer-price'],
                    'original_price': ['.original-price'],
                    'sales': ['.trade-num', '.sale-num'],
                    'shop_name': ['.company-name'],
                    'shop_link': ['.company-name a'],
                    'description': ['.offer-desc', '.content'],
                    'specs': ['.offer-attr'],
                    'images': ['.image-view img'],
                    'rating': ['.score'],
                    'comments_count': ['.comment-num'],
                    'stock': ['.stock', '.amount'],
                    'sku': ['.offer-sku'],
                    'coupon': ['.coupon-info'],
                    'promotion': ['.promotion-info']
                }
            }
        }

        self._init_browser()

    def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆä½¿ç”¨æ‚¨åŸæ¥çš„ä»£ç ï¼‰"""
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨...")
        co = ChromiumOptions()

        # è®¾ç½®æµè§ˆå™¨è·¯å¾„ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        edge_paths = [
            r'C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe',
            r'C:\Program Files\Microsoft\Edge\Application\msedge.exe',
        ]

        for path in edge_paths:
            if os.path.exists(path):
                co.set_browser_path(path)
                break

        # æµè§ˆå™¨é…ç½®
        co.set_argument('--disable-blink-features=AutomationControlled')
        # é˜²æ­¢ Chromium æ¢å¤ä¸Šæ¬¡ä¼šè¯ï¼ˆé¿å…è‡ªåŠ¨æ‰“å¼€ä¸Šæ¬¡çš„æ ‡ç­¾é¡µï¼Œä¾‹å¦‚æ·˜å®ï¼‰
        co.set_argument('--disable-restore-session-state')
        co.set_argument('--no-first-run')
        co.set_argument('--disable-gpu')
        co.set_argument('--no-sandbox')
        co.set_argument('--disable-dev-shm-usage')
        co.set_argument('--lang=zh-CN')
        co.set_argument('--disable-notifications')

        # ç”¨æˆ·ä»£ç†
        co.set_user_agent(
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'
        )

        # å¦‚æœéœ€è¦ä¿å­˜ç™»å½•ï¼Œåˆ™åˆ›å»ºå¹¶ä½¿ç”¨æŒä¹…åŒ–ç”¨æˆ·æ•°æ®ç›®å½•
        if self.use_saved_login:
            user_data_dir = os.path.join(os.path.dirname(__file__), 'domestic_browser_data')
            # ç¡®ä¿ç›®å½•å­˜åœ¨å¹¶å¯å†™
            self._ensure_user_data_dir(user_data_dir)
            co.set_user_data_path(user_data_dir)
            # æœ‰äº› Chromium å¯åŠ¨éœ€è¦æ˜¾å¼ä¼ å…¥ user-data-dir å‚æ•°
            try:
                co.set_argument(f'--user-data-dir={user_data_dir}')
                # ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶å¤¹å Defaultï¼Œå¯æ ¹æ®éœ€è¦æ›´æ”¹
                co.set_argument('--profile-directory=Default')
            except Exception:
                pass
            print(f"âœ… ä½¿ç”¨æŒä¹…åŒ–ç”¨æˆ·æ•°æ®ç›®å½•: {user_data_dir}")

            # å¦‚æœå¯ç”¨ä¿å­˜ç™»å½•ï¼Œåˆ™å¼ºåˆ¶ä½¿ç”¨å¯è§æ¨¡å¼ä»¥ä¾¿æ‰‹åŠ¨äº¤äº’å¼ç™»å½•
            if self.headless:
                print("âš ï¸ use_saved_login å·²å¯ç”¨ï¼Œå¼ºåˆ¶å…³é—­ headless æ¨¡å¼ä»¥ä¿ç•™ç™»å½•ä¿¡æ¯ï¼ˆéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰")
                self.headless = False

        # æ˜¯å¦æ— å¤´æ¨¡å¼
        if self.headless:
            co.headless()
        else:
            co.headless(False)
            co.set_argument('--start-maximized')

        try:
            self.page = ChromiumPage(addr_or_opts=co)
            # æ¸…ç©ºä»»ä½•ç”± profile æ¢å¤çš„å¯åŠ¨é¡µï¼ˆä¾‹å¦‚æœ‰æ—¶ profile ä¼šæ¢å¤ä¸Šæ¬¡æ‰“å¼€çš„æ·˜å®ï¼‰ï¼Œ
            # ç«‹å³è·³è½¬åˆ°ç©ºç™½é¡µï¼Œé¿å…åœ¨åç»­å¯¼èˆªå‰çŸ­æš‚æ˜¾ç¤ºè¿™äº›é¡µé¢ã€‚
            try:
                self.page.get('about:blank')
                time.sleep(0.3)
            except Exception:
                pass

            # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
            try:
                self.page.run_js('''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    window.chrome = { runtime: {} };
                ''')
            except Exception:
                pass

            print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

            # å¦‚æœå¯ç”¨äº†ä¿å­˜ç™»å½•ï¼Œåˆ™ç®€å•æ£€æŸ¥å½“å‰profileæ˜¯å¦éœ€è¦ç™»å½•/éªŒè¯ï¼›å¦‚éœ€è¦åˆ™æç¤ºæ‰‹åŠ¨å®Œæˆ
            if self.use_saved_login:
                try:
                    # ä¸åœ¨å¯åŠ¨æ—¶æ‰“å¼€ä»»ä½•ç«™ç‚¹æˆ–å¼ºåˆ¶æ£€æµ‹ç™»å½•çŠ¶æ€ï¼ˆæ­¤æ£€æµ‹åœ¨ä¸åŒæœºå™¨/profileä¸‹æ˜“è¯¯åˆ¤ï¼‰ã€‚
                    # æµè§ˆå™¨å·²å¯åŠ¨å¹¶åŠ è½½äº†æŒ‡å®š profileï¼›åªæœ‰åœ¨ç”¨æˆ·é€‰æ‹©ç«™ç‚¹æ—¶æ‰ä¼šå¯¼èˆªåˆ°å¯¹åº”é¡µé¢ã€‚
                    print("â„¹ï¸ ä½¿ç”¨æŒä¹…åŒ– profileï¼ˆæµè§ˆå™¨å·²å¯åŠ¨â€”â€”é€‰æ‹©ç«™ç‚¹åç¨‹åºå°†å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢ï¼›é¦–æ¬¡ç™»å½•è¯·æ‰‹åŠ¨åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­å®Œæˆä¸€æ¬¡ä»¥ä¿å­˜ä¼šè¯ï¼‰ã€‚")
                except Exception as e:
                    print(f"âš ï¸ ç™»å½•æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")

        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            exit()

    def _ensure_user_data_dir(self, path: str):
        """ç¡®ä¿ç”¨æˆ·æ•°æ®ç›®å½•å­˜åœ¨å¹¶å¯å†™"""
        try:
            if not os.path.exists(path):
                os.makedirs(path, exist_ok=True)
            # åœ¨ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªå ä½æ–‡ä»¶ï¼Œç¡®ä¿ç›®å½•æ˜¯å¯å†™çš„
            test_file = os.path.join(path, '.profile_write_test')
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write('ok')
            try:
                os.remove(test_file)
            except Exception:
                pass
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆ›å»ºç”¨æˆ·æ•°æ®ç›®å½• {path}: {e}")

    def _is_verification_or_login_present(self) -> bool:
        """ç®€å•æ£€æµ‹é¡µé¢ä¸Šæ˜¯å¦å­˜åœ¨ç™»å½•/éªŒè¯ç›¸å…³çš„ç—•è¿¹ã€‚

        è¿”å› True è¡¨ç¤ºå¯èƒ½éœ€è¦äººå·¥å¹²é¢„ï¼ˆç™»å½•/æ»‘å—/éªŒè¯ç ç­‰ï¼‰ã€‚
        æ£€æµ‹ç­–ç•¥ï¼šæ£€æŸ¥ URLï¼Œé¡µé¢æ–‡æœ¬ï¼Œå¸¸è§é€‰æ‹©å™¨ã€‚
        """
        try:
            # æ£€æŸ¥ URL ä¸­çš„æ˜æ˜¾å…³é”®è¯
            url = (self.page.url or '').lower()
            if any(k in url for k in ('login', 'signin', 'passport', 'auth', 'verify')):
                return True

            # æ£€æŸ¥é¡µé¢æ–‡æœ¬ä¸­å¸¸è§æç¤º
            text = (self.page.text or '')[:8000]
            login_keywords = ['ç™»å½•', 'è¯·ç™»å½•', 'ç™»å½•å', 'è¾“å…¥å¯†ç ', 'è¯·è¾“å…¥å¯†ç ', 'éªŒè¯ç ', 'æ»‘å—', 'è¯·å®Œæˆå®‰å…¨éªŒè¯', 'è¯·éªŒè¯']
            for kw in login_keywords:
                if kw in text:
                    return True

            # æ£€æŸ¥å¸¸è§çš„è¾“å…¥æˆ–éªŒè¯ç å…ƒç´ 
            try:
                if self.page.ele('input[type="password"]', timeout=1):
                    return True
            except:
                pass

            try:
                # captcha-like elements
                if self.page.ele('iframe[src*="captcha"]', timeout=1):
                    return True
            except:
                pass

        except Exception:
            return False

        return False

    def wait_for_manual_login(self, prompt: str = None, timeout: int = 600):
        """æç¤ºç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®Œæˆç™»å½•æˆ–éªŒè¯ï¼Œç„¶åæŒ‰ Enter ç»§ç»­ã€‚

        å‚æ•°:
            prompt: è‡ªå®šä¹‰æç¤ºä¿¡æ¯ (å¯é€‰)
            timeout: è‡ªåŠ¨æ£€æµ‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…æ—¶åå‡½æ•°è¿”å›ã€‚

        è¯´æ˜: ä»¥æœ€ç®€å•å¯é çš„æ–¹å¼å®ç°äººå·¥ç™»å½•ï¼šåœ¨å¯è§æµè§ˆå™¨ä¸­æ“ä½œåï¼ŒæŒ‰ Enter
        æˆ–ç­‰å¾…è‡ªåŠ¨æ£€æµ‹åˆ°ç™»å½•å·²è§£é™¤éªŒè¯é¡µé¢çŠ¶æ€ã€‚
        """
        if prompt is None:
            prompt = (
                "æ£€æµ‹åˆ°å¯èƒ½éœ€è¦ç™»å½•/éªŒè¯ã€‚è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®Œæˆç™»å½•æˆ–éªŒè¯ç éªŒè¯ï¼Œ"
                "å®Œæˆåå›åˆ°æ­¤æ§åˆ¶å°æŒ‰ Enter ç»§ç»­ï¼ˆæˆ–ç­‰å¾…è‡ªåŠ¨æ£€æµ‹ï¼‰ã€‚"
            )

        print('\n' + '=' * 60)
        print(prompt)
        print(f"æ­£åœ¨æ¯ 3 ç§’æ£€æµ‹ä¸€æ¬¡é¡µé¢çŠ¶æ€ï¼Œæœ€é•¿ç­‰å¾… {timeout} ç§’...\n")

        start = time.time()
        try:
            # è½®è¯¢æ£€æµ‹ï¼Œç”¨æˆ·å¯ä»¥åœ¨ä»»æ„æ—¶åˆ»æŒ‰ Enter è·³å‡º
            while True:
                # å¦‚æœéªŒè¯/ç™»å½•ç—•è¿¹æ¶ˆå¤±ï¼Œåˆ™è‡ªåŠ¨è¿”å›
                if not self._is_verification_or_login_present():
                    print("æ£€æµ‹åˆ°ç™»å½•/éªŒè¯å·²å®Œæˆï¼Œç»§ç»­çˆ¬å–...")
                    return

                # æ£€æŸ¥è¶…æ—¶
                if time.time() - start > timeout:
                    print(f"ç­‰å¾…è¶…æ—¶ ({timeout}s)ï¼Œè¯·ç¡®è®¤ç™»å½•/éªŒè¯æ˜¯å¦å®Œæˆï¼Œç„¶åæŒ‰ Enter ç»§ç»­æˆ–æ‰‹åŠ¨ç»ˆæ­¢ç¨‹åºã€‚")
                    try:
                        input('æŒ‰ Enter ç»§ç»­...')
                    except Exception:
                        pass
                    return

                # éé˜»å¡çŸ­ç­‰å¾…ï¼ŒåŒæ—¶å…è®¸ç”¨æˆ·æŒ‰ Ctrl+C é€€å‡º
                try:
                    # åœ¨ç­‰å¾…æœŸé—´ç»™ç”¨æˆ·ä¸€ä¸ªæœºä¼šæŒ‰ Enter æ¥ç«‹åˆ»ç»§ç»­
                    # ç”±äºæ™®é€š input ä¼šé˜»å¡ï¼Œè¿™é‡ŒåªåšçŸ­ç¡çœ ä»¥é¿å…é˜»å¡ä¸»çº¿ç¨‹
                    time.sleep(3)
                except KeyboardInterrupt:
                    print('ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢ç­‰å¾…')
                    return

        except Exception as e:
            print(f"æ‰‹åŠ¨ç™»å½•ç­‰å¾…å‡ºé”™: {e}")
            return

    def get_product_details_from_url(self, product_url: str) -> Dict:
        """
        ä»å•†å“URLç›´æ¥çˆ¬å–å•†å“è¯¦æƒ…

        Args:
            product_url: å•†å“è¯¦æƒ…é¡µURL

        Returns:
            å•†å“è¯¦æƒ…å­—å…¸
        """
        print(f"\nğŸ” å¼€å§‹çˆ¬å–å•†å“è¯¦æƒ…é¡µ: {product_url}")

        try:
            # 1. æ‰“å¼€å•†å“é¡µé¢
            # å…ˆå¯¼èˆªåˆ°ç©ºç™½é¡µä»¥é¿å…æ˜¾ç¤ºè¢« profile æ¢å¤çš„æ—§é¡µé¢ï¼ˆä¾‹å¦‚æ·˜å®ï¼‰ï¼Œç„¶åå†æ‰“å¼€ç›®æ ‡é¡µé¢
            try:
                self.page.get('about:blank')
                time.sleep(0.2)
            except Exception:
                pass
            self.page.get(product_url)
            time.sleep(5)  # ç­‰å¾…é¡µé¢åŠ è½½

            # å¦‚æœé¡µé¢ä¸Šå‡ºç°ç™»å½•/éªŒè¯æç¤ºï¼Œæš‚åœå¹¶è®©ç”¨æˆ·æ‰‹åŠ¨å®Œæˆ
            if self._is_verification_or_login_present():
                self.wait_for_manual_login()

            # 2. æ»šåŠ¨é¡µé¢åŠ è½½æ‰€æœ‰å†…å®¹
            self._scroll_page_gradually()

            # 3. æ£€æµ‹ç«™ç‚¹å¹¶çˆ¬å–è¯¦æƒ…
            site = self._detect_site_from_url(product_url)
            if site:
                self.current_site = site
                return self._extract_product_details(site, product_url)
            else:
                # å°è¯•è‡ªåŠ¨è¯†åˆ«ç«™ç‚¹
                return self._extract_product_details_auto(product_url)

        except Exception as e:
            print(f"âŒ çˆ¬å–å•†å“è¯¦æƒ…å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def search_and_get_details(self, site: str, keyword: str, max_items: int = 5) -> List[Dict]:
        """
        æœç´¢å…³é”®è¯å¹¶çˆ¬å–å‰Nä¸ªå•†å“çš„è¯¦æƒ…

        Args:
            site: ç«™ç‚¹åç§°
            keyword: æœç´¢å…³é”®è¯
            max_items: æœ€å¤§çˆ¬å–å•†å“æ•°é‡

        Returns:
            å•†å“è¯¦æƒ…åˆ—è¡¨
        """
        if site not in self.site_detail_configs:
            print(f"âŒ ä¸æ”¯æŒè¯¥ç«™ç‚¹: {site}")
            return []

        self.current_site = site
        site_config = self.site_detail_configs[site]

        print(f"\nğŸ” åœ¨ {site_config['name']} æœç´¢ '{keyword}'")

        try:
            # 1. æ‰§è¡Œæœç´¢
            search_url = site_config['search_url'].format(keyword=quote(keyword))
            # å…ˆæ¸…ç©ºå½“å‰é¡µé¢ï¼ˆé¿å…æ˜¾ç¤º profile æ¢å¤çš„é¡µé¢ï¼‰ï¼Œç„¶åå†å¯¼èˆªåˆ°ç›®æ ‡æœç´¢é¡µ
            try:
                self.page.get('about:blank')
                time.sleep(0.2)
            except Exception:
                pass
            self.page.get(search_url)
            time.sleep(6)

            # å¦‚æœéœ€è¦ç™»å½•/éªŒè¯ï¼Œæš‚åœå¹¶è®©ç”¨æˆ·æ‰‹åŠ¨å®Œæˆ
            if self._is_verification_or_login_present():
                self.wait_for_manual_login()

            # æ»šåŠ¨åŠ è½½æ›´å¤šå•†å“
            self._scroll_page_gradually()

            # 2. æå–å•†å“é“¾æ¥
            product_urls = self._extract_product_urls_from_search(site)
            print(f"âœ… æ‰¾åˆ° {len(product_urls)} ä¸ªå•†å“é“¾æ¥")

            if not product_urls:
                print("âš ï¸ æœªæ‰¾åˆ°å•†å“é“¾æ¥")
                return []

            # 3. é™åˆ¶çˆ¬å–æ•°é‡
            product_urls = product_urls[:max_items]

            # 4. é€ä¸ªçˆ¬å–å•†å“è¯¦æƒ…
            all_details = []
            for i, url in enumerate(product_urls, 1):
                print(f"\nğŸ“¦ æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(product_urls)} ä¸ªå•†å“...")

                # å®Œæ•´çš„URL
                if not url.startswith('http'):
                    if url.startswith('//'):
                        url = 'https:' + url
                    else:
                        url = urljoin(self.page.url, url)

                # çˆ¬å–è¯¦æƒ…
                details = self.get_product_details_from_url(url)
                if details:
                    all_details.append(details)
                    print(f"âœ… æˆåŠŸçˆ¬å–: {details.get('title', 'æœªçŸ¥')[:50]}...")

                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
                if i < len(product_urls):
                    delay = random.uniform(3, 8)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åçˆ¬å–ä¸‹ä¸€ä¸ªå•†å“...")
                    time.sleep(delay)

            print(f"\nğŸ‰ å®Œæˆï¼æˆåŠŸçˆ¬å– {len(all_details)} ä¸ªå•†å“çš„è¯¦æƒ…")
            return all_details

        except Exception as e:
            print(f"âŒ æœç´¢çˆ¬å–å¤±è´¥: {e}")
            return []

    def _extract_product_urls_from_search(self, site: str) -> List[str]:
        """
        ä»æœç´¢ç»“æœé¡µæå–å•†å“é“¾æ¥
        """
        urls = []

        # è·å–é¡µé¢æ‰€æœ‰é“¾æ¥
        try:
            # ä¼˜å…ˆé’ˆå¯¹äº¬ä¸œåšä¸“é—¨å¤„ç†ï¼šå¤šç­–ç•¥å°è¯•æå–å•†å“é“¾æ¥ï¼ˆli[data-sku]ã€å¸¸ç”¨å•†å“é“¾æ¥é€‰æ‹©å™¨ã€ä»¥åŠé€šç”¨çš„ JS æ‰«æï¼‰
            if site == 'jd':
                try:
                    # æ–¹æ³•1ï¼šli[data-sku]
                    sku_elements = self.page.eles('css:li[data-sku]') or self.page.eles('xpath://li[@data-sku]')
                    if sku_elements:
                        for el in sku_elements:
                            try:
                                sku = el.attr('data-sku')
                                if sku:
                                    urls.append(f'https://item.jd.com/{sku}.html')
                            except:
                                continue
                        urls = list(dict.fromkeys(urls))
                        if urls:
                            return urls

                    # æ–¹æ³•2ï¼šå¸¸è§çš„å•†å“é“¾æ¥é€‰æ‹©å™¨ï¼ˆæ¯”å¦‚ .p-name a / .p-img aï¼‰
                    try:
                        name_links = self.page.eles('css:.p-name a') or self.page.eles('css:.p-img a') or self.page.eles('css:.p-name a.J_ClickStat')
                        if name_links:
                            for a in name_links:
                                try:
                                    href = a.attr('href') or ''
                                    if href:
                                        if href.startswith('//'):
                                            href = 'https:' + href
                                        elif href.startswith('/'):
                                            href = urljoin(self.page.url, href)
                                        if 'item.jd.com' in href or re.search(r'/\d+\.html', href):
                                            urls.append(href)
                                except:
                                    continue
                            urls = list(dict.fromkeys(urls))
                            if urls:
                                return urls
                    except Exception:
                        pass

                    # æ–¹æ³•3ï¼šè¿è¡Œ JS æ‰«æé¡µé¢ä¸Šæ‰€æœ‰é“¾æ¥ï¼Œæ”¶é›†å¸¦ item.jd.com æˆ– åŒ¹é… item id çš„é“¾æ¥
                    try:
                        js_collect = '''
                        (function(){
                            var hrefs = Array.from(document.querySelectorAll('a')).map(a=>a.href || a.getAttribute('href')||'');
                            hrefs = hrefs.filter(function(h){ if(!h) return false; return h.indexOf('item.jd.com')!==-1 || /\\/\\d+\\.html/.test(h); });
                            // è§„èŒƒåŒ–åè®®-ç›¸å¯¹é“¾æ¥
                            hrefs = hrefs.map(function(h){ if(h.indexOf('http')!==0 && h.indexOf('//')===0) return 'https:'+h; return h; });
                            return Array.from(new Set(hrefs));
                        })();
                        '''
                        try:
                            collected = self.page.run_js(js_collect) or []
                        except Exception:
                            collected = []
                        if collected:
                            for h in collected:
                                try:
                                    if h and isinstance(h, str):
                                        urls.append(h)
                                except:
                                    continue
                            urls = list(dict.fromkeys(urls))
                            if urls:
                                return urls
                    except Exception:
                        pass

                except Exception:
                    # è‹¥ä¸“é—¨å¤„ç†å¤±è´¥ï¼Œåˆ™é€€å›åˆ°é€šç”¨æ–¹æ³•
                    pass

            all_links = self.page.eles('tag:a')

            # æ ¹æ®ç«™ç‚¹æ¨¡å¼åŒ¹é…
            patterns = self.site_detail_configs[site]['item_url_patterns']

            for link in all_links:
                try:
                    href = link.attr('href') or ''
                    if not href:
                        continue

                    # è§„èŒƒåŒ– hrefï¼šå¤„ç† // å¼€å¤´å’Œç›¸å¯¹é“¾æ¥
                    if href.startswith('//'):
                        href_norm = 'https:' + href
                    elif href.startswith('/'):
                        try:
                            href_norm = urljoin(self.page.url, href)
                        except:
                            href_norm = href
                    else:
                        href_norm = href

                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…å•†å“URLæ¨¡å¼æˆ–ç®€å•åŒ…å«å…³é”®åŸŸå
                    matched = False
                    for pattern in patterns:
                        try:
                            if re.search(pattern, href_norm, re.IGNORECASE):
                                urls.append(href_norm)
                                matched = True
                                break
                        except:
                            continue

                    # é¢å¤–å¢å¼ºåˆ¤æ–­ï¼šå¯¹äºäº¬ä¸œï¼Œå¦‚æœ href ä¸­åŒ…å« 'item.jd.com' ä¹Ÿç®—
                    if (not matched) and site == 'jd' and 'item.jd.com' in href_norm:
                        urls.append(href_norm)

                except Exception:
                    continue

            # å»é‡
            urls = list(dict.fromkeys(urls))

        except Exception as e:
            print(f"æå–å•†å“é“¾æ¥å¤±è´¥: {e}")

        return urls

    def _extract_product_details(self, site: str, product_url: str) -> Dict:
        """
        æå–å•†å“è¯¦ç»†ä¿¡æ¯
        """
        details = {
            'platform': self.site_detail_configs[site]['name'],
            'url': product_url,
            'title': '',
            'price': '',
            'original_price': '',
            'sales': '',
            'shop_name': '',
            'shop_url': '',
            'description': '',
            'specifications': {},
            'images': [],
            'rating': '',
            'comments_count': '',
            'stock': '',
            'sku_info': '',
            'coupons': [],
            'promotions': [],
            'crawl_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        # æ ¹æ®ç«™ç‚¹é…ç½®æå–ä¿¡æ¯
        selectors = self.site_detail_configs[site]['detail_selectors']

        # 1. æå–æ ‡é¢˜
        details['title'] = self._extract_with_selectors(selectors.get('title', []))

        # 2. æå–ä»·æ ¼ï¼ˆç‰¹åˆ«å¤„ç†äº¬ä¸œï¼‰
        if site == 'jd':
            # æå–å•†å“IDç”¨äºäº¬ä¸œä»·æ ¼é€‰æ‹©å™¨
            item_id = self._extract_jd_item_id(product_url)
            if item_id:
                # äº¬ä¸œä»·æ ¼æ˜¯åŠ¨æ€çš„ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                price_selector = f'.J-p-{item_id}'
                details['price'] = self._extract_price_jd_special(price_selector)

        if not details['price']:
            details['price'] = self._extract_with_selectors(selectors.get('price', []))

        # 3. æå–åŸä»·
        details['original_price'] = self._extract_with_selectors(selectors.get('original_price', []))

        # 4. æå–é”€é‡
        details['sales'] = self._extract_with_selectors(selectors.get('sales', []))

        # 5. æå–åº—é“ºä¿¡æ¯
        details['shop_name'] = self._extract_with_selectors(selectors.get('shop_name', []))
        details['shop_url'] = self._extract_link_with_selectors(selectors.get('shop_link', []))

        # 6. æå–æè¿°ï¼ˆå¯èƒ½éœ€è¦ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…ï¼‰
        details['description'] = self._extract_description(site)

        # 7. æå–è§„æ ¼å‚æ•°
        details['specifications'] = self._extract_specifications(selectors.get('specs', []))

        # 8. æå–å›¾ç‰‡
        details['images'] = self._extract_images(selectors.get('images', []))

        # 9. æå–å…¶ä»–ä¿¡æ¯
        details['rating'] = self._extract_with_selectors(selectors.get('rating', []))
        details['comments_count'] = self._extract_with_selectors(selectors.get('comments_count', []))
        details['stock'] = self._extract_with_selectors(selectors.get('stock', []))
        details['sku_info'] = self._extract_with_selectors(selectors.get('sku', []))

        # 10. æ¸…ç†å’Œæ ¼å¼åŒ–æ•°æ®
        details = self._clean_details(details)
        # è¡¥å…… style/color/discount æ–¹ä¾¿åç»­ API ä½¿ç”¨
        details = self._enrich_details(details)

        return details

    def _extract_with_selectors(self, selectors: List[str]) -> str:
        """ä½¿ç”¨å¤šä¸ªé€‰æ‹©å™¨å°è¯•æå–æ–‡æœ¬"""
        for selector in selectors:
            try:
                element = self.page.ele(selector, timeout=2)
                if element:
                    text = element.text.strip()
                    if text:
                        return text
            except:
                continue
        return ''

    def _extract_link_with_selectors(self, selectors: List[str]) -> str:
        """ä½¿ç”¨å¤šä¸ªé€‰æ‹©å™¨å°è¯•æå–é“¾æ¥"""
        for selector in selectors:
            try:
                element = self.page.ele(selector, timeout=2)
                if element:
                    href = element.attr('href')
                    if href:
                        if href.startswith('//'):
                            return 'https:' + href
                        elif not href.startswith('http'):
                            return urljoin(self.page.url, href)
                        else:
                            return href
            except:
                continue
        return ''

    def _extract_price_jd_special(self, selector: str) -> str:
        """ç‰¹æ®Šå¤„ç†äº¬ä¸œä»·æ ¼ï¼ˆäº¬ä¸œä»·æ ¼ç»å¸¸æ˜¯åŠ¨æ€åŠ è½½çš„ï¼‰"""
        try:
            # æ–¹æ³•1ï¼šç›´æ¥é€‰æ‹©å™¨
            element = self.page.ele(selector, timeout=3)
            if element:
                return element.text.strip()

            # æ–¹æ³•2ï¼šæŸ¥æ‰¾ä»·æ ¼ç›¸å…³çš„å…ƒç´ 
            price_elements = self.page.eles('.price, [class*="price"], [class*="Price"]')
            for elem in price_elements:
                text = elem.text.strip()
                if text and any(char in text for char in ['Â¥', 'ï¿¥', '.']):
                    # æå–æ•°å­—ä»·æ ¼
                    match = re.search(r'[\d.,]+', text)
                    if match:
                        return match.group()

            # æ–¹æ³•3ï¼šåœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢ä»·æ ¼
            page_text = self.page.text
            price_patterns = [
                r'Â¥\s*([\d\.,]+)',
                r'ï¿¥\s*([\d\.,]+)',
                r'äº¬ä¸œä»·[:ï¼š]\s*([\d\.,]+)'
            ]

            for pattern in price_patterns:
                match = re.search(pattern, page_text)
                if match:
                    return match.group(1)

        except Exception as e:
            print(f"æå–äº¬ä¸œä»·æ ¼å¤±è´¥: {e}")

        return ''

    def _extract_description(self, site: str) -> str:
        """æå–å•†å“æè¿°ï¼ˆå¯èƒ½éœ€è¦äº¤äº’ï¼‰"""
        description = ''

        try:
            # å°è¯•ç‚¹å‡»"æŸ¥çœ‹è¯¦æƒ…"ç­‰æŒ‰é’®
            detail_buttons = [
                'æŸ¥çœ‹è¯¦æƒ…', 'å•†å“è¯¦æƒ…', 'å›¾æ–‡è¯¦æƒ…',
                'æŸ¥çœ‹å›¾æ–‡è¯¦æƒ…', 'äº§å“è¯¦æƒ…', 'è¯¦æƒ…ä»‹ç»'
            ]

            for button_text in detail_buttons:
                try:
                    button = self.page.ele(f'text:{button_text}', timeout=2)
                    if button:
                        button.click()
                        time.sleep(3)
                        break
                except:
                    continue

            # å°è¯•æå–æè¿°å†…å®¹
            if site == 'jd':
                # äº¬ä¸œæè¿°åœ¨iframeä¸­
                try:
                    iframe = self.page.ele('#product-detail iframe', timeout=3)
                    if iframe:
                        # åˆ‡æ¢åˆ°iframe
                        self.page.switch_to_frame(iframe)
                        desc_element = self.page.ele('body', timeout=3)
                        if desc_element:
                            description = desc_element.text[:2000]  # é™åˆ¶é•¿åº¦
                        self.page.switch_to_frame()
                except:
                    pass

            # é€šç”¨æè¿°æå–
            desc_selectors = [
                '.detail-content', '.product-detail', '.desc-content',
                '#description', '.tb-detail-content'
            ]

            for selector in desc_selectors:
                try:
                    element = self.page.ele(selector, timeout=2)
                    if element:
                        description = element.text[:2000]
                        break
                except:
                    continue

        except Exception as e:
            print(f"æå–æè¿°å¤±è´¥: {e}")

        return description

    def _extract_specifications(self, selectors: List[str]) -> Dict:
        """æå–è§„æ ¼å‚æ•°"""
        specs = {}

        for selector in selectors:
            try:
                spec_elements = self.page.eles(selector)
                for element in spec_elements:
                    text = element.text.strip()
                    if text and 'ï¼š' in text:
                        # è§£æé”®å€¼å¯¹
                        lines = text.split('\n')
                        for line in lines:
                            if 'ï¼š' in line:
                                key, value = line.split('ï¼š', 1)
                                specs[key.strip()] = value.strip()
                    elif ':' in text:
                        # è‹±æ–‡å†’å·åˆ†éš”
                        lines = text.split('\n')
                        for line in lines:
                            if ':' in line:
                                key, value = line.split(':', 1)
                                specs[key.strip()] = value.strip()
            except:
                continue

        return specs

    def _extract_images(self, selectors: List[str]) -> List[str]:
        """æå–å•†å“å›¾ç‰‡"""
        images = []

        for selector in selectors:
            try:
                img_elements = self.page.eles(selector)
                for img in img_elements:
                    src = img.attr('src') or img.attr('data-src') or img.attr('data-original')
                    if src:
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif not src.startswith('http'):
                            src = urljoin(self.page.url, src)
                        images.append(src)
            except:
                continue

        return images

    def _scroll_page_gradually(self):
        """é€æ­¥æ»šåŠ¨é¡µé¢"""
        print("æ»šåŠ¨é¡µé¢åŠ è½½å†…å®¹...")

        # å¤šæ¬¡æ»šåŠ¨
        for i in range(5):
            try:
                scroll_distance = 500 + i * 200
                self.page.scroll.down(scroll_distance)
                time.sleep(1.5 + i * 0.3)
            except:
                pass

    def _detect_site_from_url(self, url: str) -> Optional[str]:
        """ä»URLæ£€æµ‹ç«™ç‚¹"""
        for site, config in self.site_detail_configs.items():
            for pattern in config['item_url_patterns']:
                if re.search(pattern, url, re.IGNORECASE):
                    return site
        return None

    def _extract_jd_item_id(self, url: str) -> Optional[str]:
        """ä»äº¬ä¸œURLæå–å•†å“ID"""
        match = re.search(r'item\.jd\.com/(\d+)\.html', url)
        if match:
            return match.group(1)
        return None

    def _extract_product_details_auto(self, product_url: str) -> Dict:
        """è‡ªåŠ¨æå–å•†å“è¯¦æƒ…ï¼ˆå½“æ— æ³•è¯†åˆ«ç«™ç‚¹æ—¶ä½¿ç”¨ï¼‰"""
        print("âš ï¸ æ— æ³•è¯†åˆ«ç«™ç‚¹ï¼Œä½¿ç”¨é€šç”¨æå–æ–¹æ³•")

        details = {
            'platform': 'æœªçŸ¥',
            'url': product_url,
            'title': '',
            'price': '',
            'description': '',
            'crawl_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        try:
            # é€šç”¨é€‰æ‹©å™¨å°è¯•æå–
            # 1. æ ‡é¢˜
            title_selectors = ['h1', '.title', '.product-title', '.goods-title', '[class*="title"]']
            details['title'] = self._extract_with_selectors(title_selectors)

            # 2. ä»·æ ¼
            price_selectors = [
                '.price', '.product-price', '.goods-price',
                '[class*="price"]', '[class*="Price"]'
            ]
            details['price'] = self._extract_with_selectors(price_selectors)

            # 3. æè¿°
            desc_selectors = ['.description', '.product-desc', '.goods-desc', '[class*="desc"]']
            details['description'] = self._extract_with_selectors(desc_selectors)

            # 4. å›¾ç‰‡
            img_selectors = ['.main-img img', '.product-img img', '.goods-img img']
            details['images'] = self._extract_images(img_selectors)

        except Exception as e:
            print(f"é€šç”¨æå–å¤±è´¥: {e}")

        return details

    def _clean_details(self, details: Dict) -> Dict:
        """æ¸…ç†å’Œæ ¼å¼åŒ–è¯¦æƒ…æ•°æ®"""
        # æ¸…ç†ä»·æ ¼
        for price_field in ['price', 'original_price']:
            if details.get(price_field):
                # æå–æ•°å­—
                match = re.search(r'[\d.,]+', details[price_field])
                if match:
                    details[price_field] = match.group().replace(',', '')

        # æ¸…ç†é”€é‡
        if details.get('sales'):
            # æå–æ•°å­—
            match = re.search(r'[\d.]+' , details['sales'])
            if match:
                details['sales'] = match.group()

        # æ¸…ç†æ ‡é¢˜å’Œæè¿°
        if details.get('title'):
            details['title'] = details['title'].strip()
            if len(details['title']) > 200:
                details['title'] = details['title'][:197] + '...'

        if details.get('description'):
            details['description'] = details['description'].strip()
            if len(details['description']) > 3000:
                details['description'] = details['description'][:2997] + '...'

        return details

    def _to_float_price(self, price_str: str) -> Optional[float]:
        """æŠŠä»·æ ¼å­—ç¬¦ä¸²è½¬æ¢ä¸º floatï¼Œå¤±è´¥è¿”å› Noneã€‚"""
        try:
            if not price_str:
                return None
            m = re.search(r'[\d.,]+', str(price_str))
            if not m:
                return None
            num = m.group().replace(',', '')
            return float(num)
        except Exception:
            return None

    def _enrich_details(self, details: Dict) -> Dict:
        """ä»è§„æ ¼å‚æ•°å’Œä»·æ ¼æ¨æ–­å‡º style/color/discount ç­‰ä¾¿äºåç»­ API ä½¿ç”¨çš„å­—æ®µã€‚"""
        try:
            specs = details.get('specifications') or {}

            def find_in_specs(keys):
                for key in keys:
                    for k, v in specs.items():
                        if key in k or key in str(v):
                            return v
                return ''

            # é£æ ¼/æ ·å¼
            style = find_in_specs(['é£æ ¼', 'æ ·å¼', 'æ¬¾å¼', 'style']) or ''
            # é¢œè‰²
            color = find_in_specs(['é¢œè‰²', 'è‰²ç³»', 'é¢œè‰²åˆ†ç±»', 'color']) or ''

            # æŠ˜æ‰£ï¼šä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„ä¿ƒé”€/ä¼˜æƒ ä¿¡æ¯ï¼Œå¦åˆ™é€šè¿‡åŸä»·å’Œå½“å‰ä»·è®¡ç®—
            discount = ''
            # æŸ¥çœ‹ coupons/promotions å­—æ®µ
            coupons = details.get('coupons') or []
            promotions = details.get('promotions') or []
            if coupons:
                discount = ';'.join([str(c) for c in coupons])
            elif promotions:
                discount = ';'.join([str(p) for p in promotions])
            else:
                # è®¡ç®—åŸºäºåŸä»·å’Œä»·æ ¼çš„é™å¹…
                p = self._to_float_price(details.get('price', ''))
                op = self._to_float_price(details.get('original_price', ''))
                if p and op and op > 0 and p < op:
                    percent_off = round((1 - (p / op)) * 100, 1)
                    discount = f"{percent_off}%"

            # å°†æ–°å­—æ®µå†™å› details
            details['style'] = style
            details['color'] = color
            details['discount'] = discount

            return details
        except Exception:
            return details

    def save_details(self, products: List[Dict], filename: str = None):
        """ä¿å­˜å•†å“è¯¦æƒ…"""
        if not products:
            print("âš ï¸ æ²¡æœ‰å•†å“æ•°æ®å¯ä¿å­˜")
            return

        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"product_details_{timestamp}.json"

        try:
            # ä¿å­˜ä¸ºJSON
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)

            print(f"âœ… å•†å“è¯¦æƒ…å·²ä¿å­˜åˆ°: {filename}")

            # åŒæ—¶ä¿å­˜ä¸ºCSV
            csv_filename = filename.replace('.json', '.csv')
            self._save_details_to_csv(products, csv_filename)

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

    def _save_details_to_csv(self, products: List[Dict], filename: str):
        """ä¿å­˜å•†å“è¯¦æƒ…ä¸ºCSV"""
        try:
            if not products:
                return

            # å‡†å¤‡CSVæ•°æ®
            csv_data = []
            for product in products:
                # è§„æ ¼å‚æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                specs_str = '; '.join([f'{k}: {v}' for k, v in product.get('specifications', {}).items()])
                images_str = '; '.join(product.get('images', [])[:3])  # åªä¿å­˜å‰3å¼ å›¾ç‰‡

                csv_row = {
                    'å¹³å°': product.get('platform', ''),
                    'å•†å“æ ‡é¢˜': product.get('title', ''),
                    'ä»·æ ¼': product.get('price', ''),
                    'åŸä»·': product.get('original_price', ''),
                    'é”€é‡': product.get('sales', ''),
                    'åº—é“ºåç§°': product.get('shop_name', ''),
                    'åº—é“ºé“¾æ¥': product.get('shop_url', ''),
                    'æè¿°': product.get('description', '')[:500],  # é™åˆ¶é•¿åº¦
                    'è§„æ ¼å‚æ•°': specs_str,
                    'è¯„åˆ†': product.get('rating', ''),
                    'è¯„è®ºæ•°': product.get('comments_count', ''),
                    'åº“å­˜': product.get('stock', ''),
                    'å›¾ç‰‡': images_str,
                    'å•†å“é“¾æ¥': product.get('url', ''),
                    'çˆ¬å–æ—¶é—´': product.get('crawl_time', ''),
                    'é£æ ¼/æ ·å¼': product.get('style', ''),
                    'é¢œè‰²': product.get('color', ''),
                    'æŠ˜æ‰£': product.get('discount', '')
                }
                csv_data.append(csv_row)

            # å†™å…¥CSV
            with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                fieldnames = list(csv_data[0].keys()) if csv_data else []
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(csv_data)

            print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {filename}")

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜CSVå¤±è´¥: {e}")

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            try:
                self.page.quit()
                print("âœ… æµè§ˆå™¨å·²å…³é—­")
            except:
                pass

    def _is_logged_in_site(self, site: str) -> bool:
        """åŸºäºç«™ç‚¹ç‰¹å®šçš„é¡µé¢å…ƒç´ åˆ¤æ–­æ˜¯å¦å·²ç™»å½•ï¼Œè¿”å› True è¡¨ç¤ºå·²ç™»å½•ã€‚"""
        try:
            # ç»™é¡µé¢ä¸€äº›æ—¶é—´æ¸²æŸ“åŠ¨æ€å†…å®¹
            time.sleep(2)

            page_text = (self.page.text or '')[:5000]

            # å°è¯•é€šè¿‡ document.cookie åˆ¤æ–­ï¼ˆæ›´å¯é ï¼‰ï¼Œéœ€è¦æµè§ˆå™¨é¡µé¢å·²åŠ è½½
            cookies_str = ''
            try:
                cookies_str = self.page.run_js("return document.cookie") or ''
            except Exception:
                cookies_str = ''

            if site == 'taobao':
                # Cookie-based check for Taobao
                if cookies_str and any(k in cookies_str for k in ['cookie2', 'cna', 't', 'unb', 'thw']):
                    return True
                # æœªç™»å½•é¡µé¢é€šå¸¸åŒ…å«æç¤º 'äº²ï¼Œè¯·ç™»å½•' æˆ– 'è¯·ç™»å½•'
                if 'äº²ï¼Œè¯·ç™»å½•' in page_text or 'è¯·ç™»å½•' in page_text:
                    return False

                # å·²ç™»å½•æ—¶ä¼šæœ‰ 'æˆ‘çš„æ·˜å®'ã€ç”¨æˆ·åæˆ–ç”¨æˆ·å¤´åƒï¼ˆ'æˆ‘çš„æ·˜å®' æ˜¯è¾ƒç¨³å¥çš„ä¿¡å·ï¼‰
                if 'æˆ‘çš„æ·˜å®' in page_text or 'æˆ‘çš„è´­ç‰©è½¦' in page_text:
                    return True

                # å°è¯•é€šè¿‡å…ƒç´ è¯†åˆ«ï¼šä¼˜å…ˆæŸ¥æ‰¾ç”¨æˆ·èœå•æˆ–ç”¨æˆ·å
                try:
                    user_ele = self.page.ele('css:.site-nav-user, css:#J_MyTaobao, text:æˆ‘çš„æ·˜å®', timeout=2)
                    if user_ele:
                        # è‹¥å…ƒç´ å­˜åœ¨ä¸”æœ‰æ–‡æœ¬ï¼Œä¸”ä¸æ˜¯ç™»å½•æç¤ºï¼Œåˆ™è§†ä¸ºå·²ç™»å½•
                        txt = (user_ele.text or '').strip()
                        if txt and 'ç™»å½•' not in txt:
                            return True
                        # å…ƒç´ å­˜åœ¨ä½†æ²¡æœ‰æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯å¤´åƒï¼‰ï¼Œä¹Ÿå¯è®¤ä¸ºå·²ç™»å½•
                        if not txt:
                            return True
                except Exception:
                    pass

                return False

            elif site == 'jd':
                # Cookie-based check for JD (pt_key/pt_pin æˆ– unick/pin è¡¨ç¤ºç™»å½•)
                if cookies_str and any(k in cookies_str for k in ['pt_key', 'pt_pin', 'unick', 'pin']):
                    return True
                # äº¬ä¸œæœªç™»å½•é¡¶éƒ¨é€šå¸¸æ˜¾ç¤º 'è¯·ç™»å½•'ï¼Œå·²ç™»å½•ä¼šæ˜¾ç¤ºç”¨æˆ·åæˆ–'æ‚¨å¥½'
                if 'è¯·ç™»å½•' in page_text or 'ç™»å½•' in page_text and 'æˆ‘çš„äº¬ä¸œ' not in page_text:
                    # ambiguous: check element text
                    pass

                try:
                    tt = self.page.ele('css:#ttbar-login', timeout=2)
                    if tt and tt.text:
                        txt = tt.text.strip()
                        # å¦‚æœåŒ…å«'è¯·ç™»å½•'æˆ–'ç™»å½•'åˆ™è¡¨ç¤ºæœªç™»å½•
                        if 'è¯·ç™»å½•' in txt or 'ç™»å½•' in txt:
                            return False
                        # å¦åˆ™åŒ…å«ç”¨æˆ·åæˆ–é—®å€™è¯­ï¼Œè§†ä¸ºå·²ç™»å½•
                        return True
                except Exception:
                    pass

                # æ£€æŸ¥å¸¸è§ç”¨æˆ·åå…ƒç´ 
                try:
                    nick = self.page.ele('css:.nickname', timeout=2)
                    if nick and (nick.text or '').strip():
                        return True
                except Exception:
                    pass

                # æœ€ååŸºäºé¡µé¢æ–‡æœ¬çš„å¯å‘å¼åˆ¤æ–­
                if 'æ‚¨å¥½' in page_text or 'æˆ‘çš„è®¢å•' in page_text or 'æˆ‘çš„äº¬ä¸œ' in page_text:
                    return True

                return False

            else:
                return False

        except Exception:
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å›½å†…ç”µå•†å¹³å°å•†å“è¯¦æƒ…çˆ¬è™«")
    print("=" * 60)
    print("æ”¯æŒï¼šç›´æ¥çˆ¬å–å•†å“è¯¦æƒ…é¡µæˆ–æœç´¢åçˆ¬å–")
    print("=" * 60)

    try:
        # åˆ›å»ºçˆ¬è™«
        crawler = ChineseEcommerceDetailCrawler(
            headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            use_saved_login=True
        )

        print("\nè¯·é€‰æ‹©çˆ¬å–æ–¹å¼ï¼š")
        print("1. ç›´æ¥è¾“å…¥å•†å“URLçˆ¬å–")
        print("2. æœç´¢å…³é”®è¯åçˆ¬å–å•†å“è¯¦æƒ…")
        print("3. æ‰¹é‡çˆ¬å–å¤šä¸ªå•†å“")

        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()

        if choice == '1':
            # æ–¹å¼1ï¼šç›´æ¥è¾“å…¥URL
            product_url = input("\nè¯·è¾“å…¥å•†å“è¯¦æƒ…é¡µURL: ").strip()
            if product_url:
                details = crawler.get_product_details_from_url(product_url)
                if details:
                    print(f"\nâœ… å•†å“è¯¦æƒ…çˆ¬å–æˆåŠŸï¼")
                    print(f"æ ‡é¢˜: {details.get('title')}")
                    print(f"ä»·æ ¼: {details.get('price')}")
                    print(f"é”€é‡: {details.get('sales')}")
                    print(f"åº—é“º: {details.get('shop_name')}")
                    crawler.save_details([details])

        elif choice == '2':
            # æ–¹å¼2ï¼šæœç´¢åçˆ¬å–
            print("\nè¯·é€‰æ‹©å¹³å°ï¼š")
            print("1. äº¬ä¸œ")
            print("2. æ·˜å®")
            print("3. å¤©çŒ«")
            print("4. 1688")

            site_choice = input("è¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            site_map = {'1': 'jd', '2': 'taobao', '3': 'tmall', '4': '1688'}
            site = site_map.get(site_choice, 'jd')

            keyword = input(f"\nè¯·è¾“å…¥åœ¨ {crawler.site_detail_configs[site]['name']} æœç´¢çš„å…³é”®è¯: ").strip()
            if not keyword:
                keyword = "æ‰‹æœº"  # é»˜è®¤å…³é”®è¯

            max_items = input("è¯·è¾“å…¥æœ€å¤šçˆ¬å–å•†å“æ•°é‡ (é»˜è®¤5): ").strip()
            max_items = int(max_items) if max_items.isdigit() else 5

            # æ‰§è¡Œæœç´¢å¹¶çˆ¬å–è¯¦æƒ…
            details_list = crawler.search_and_get_details(site, keyword, max_items)

            # æ‰“å°ç»“æœ
            if details_list:
                print(f"\nâœ… æˆåŠŸçˆ¬å– {len(details_list)} ä¸ªå•†å“çš„è¯¦æƒ…")
                for details in details_list:
                    print(f"æ ‡é¢˜: {details.get('title')}, ä»·æ ¼: {details.get('price')}, åº—é“º: {details.get('shop_name')}")
                # è‡ªåŠ¨ä¿å­˜ä¸º JSON å’Œ CSV
                try:
                    crawler.save_details(details_list)
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å•†å“")

        elif choice == '3':
            # æ–¹å¼3ï¼šæ‰¹é‡çˆ¬å–
            file_path = input("\nè¯·è¾“å…¥åŒ…å«å•†å“é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„: ").strip()
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f.readlines() if line.strip()]

                all_details = []
                for i, url in enumerate(urls, 1):
                    print(f"\nğŸ“¦ æ­£åœ¨çˆ¬å–ç¬¬ {i}/{len(urls)} ä¸ªå•†å“...")
                    details = crawler.get_product_details_from_url(url)
                    if details:
                        all_details.append(details)
                        print(f"âœ… æˆåŠŸçˆ¬å–: {details.get('title', 'æœªçŸ¥')[:50]}...")

                    # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
                    if i < len(urls):
                        delay = random.uniform(3, 8)
                        print(f"â³ ç­‰å¾… {delay:.1f} ç§’åçˆ¬å–ä¸‹ä¸€ä¸ªå•†å“...")
                        time.sleep(delay)

                print(f"\nğŸ‰ å®Œæˆï¼æˆåŠŸçˆ¬å– {len(all_details)} ä¸ªå•†å“çš„è¯¦æƒ…")
                # ä¿å­˜ç»“æœ
                crawler.save_details(all_details)
            else:
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

    finally:
        # ç¡®ä¿å…³é—­æµè§ˆå™¨ï¼ˆä»…å½“ crawler å·²æˆåŠŸåˆ›å»ºæ—¶ï¼‰
        try:
            if 'crawler' in locals() and locals().get('crawler'):
                locals().get('crawler').close()
        except Exception:
            pass


if __name__ == "__main__":
    main()
