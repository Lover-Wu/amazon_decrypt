"""
Amazon å•†å“æœç´¢è¯¦æƒ…ä¸€ä½“åŒ–çˆ¬è™«
åŸºäº DrissionPage åº“å®ç°
ç›´æ¥é€šè¿‡å…³é”®è¯æœç´¢å¹¶çˆ¬å–å•†å“è¯¦æƒ…ä¿¡æ¯
"""
import time
import os
import json
import csv
import re
from typing import List, Dict, Optional
from urllib.parse import urljoin
from pathlib import Path
from DrissionPage import ChromiumPage, ChromiumOptions


class AmazonSearchDetailCrawler:
    """Amazon å•†å“æœç´¢è¯¦æƒ…ä¸€ä½“åŒ–çˆ¬è™«ç±»"""

    def __init__(self, headless: bool = False, use_saved_login: bool = True):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            use_saved_login: æ˜¯å¦ä½¿ç”¨ä¿å­˜çš„ç™»å½•ä¿¡æ¯
        """
        self.page = None
        self.headless = headless
        self.use_saved_login = use_saved_login
        self.base_url = "https://www.amazon.com"
        self._init_browser()

    def _init_browser(self):
        """åˆå§‹åŒ–Edgeæµè§ˆå™¨é…ç½®"""
        print("ğŸš€ å¯åŠ¨ Microsoft Edge æµè§ˆå™¨...")
        co = ChromiumOptions()

        # Edgeæµè§ˆå™¨è·¯å¾„
        edge_paths = [
            r'C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe',
            r'C:\Program Files\Microsoft\Edge\Application\msedge.exe',
            os.path.expanduser(r'~\AppData\Local\Microsoft\Edge\Application\msedge.exe'),
        ]

        # è‡ªåŠ¨æŸ¥æ‰¾Edgeè·¯å¾„
        edge_found = False
        for path in edge_paths:
            if os.path.exists(path):
                co.set_browser_path(path)
                edge_found = True
                print(f"âœ… æ‰¾åˆ° Microsoft Edge: {path}")
                break

        if not edge_found:
            print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° Microsoft Edge æµè§ˆå™¨ï¼")
            print("è¯·ç¡®ä¿æ‚¨çš„Windowsç³»ç»Ÿå·²å®‰è£… Microsoft Edgeã€‚")
            input("\næŒ‰ Enter é”®é€€å‡º...")
            exit()

        # Edgeæµè§ˆå™¨é…ç½®
        co.set_argument('--disable-blink-features=AutomationControlled')
        co.set_argument('--disable-gpu')
        co.set_argument('--no-sandbox')
        co.set_argument('--disable-dev-shm-usage')
        co.set_argument('--lang=en-US')

        # Edgeç”¨æˆ·ä»£ç†
        co.set_user_agent(
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
            '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        )

        # æ˜¯å¦ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•
        if self.use_saved_login:
            # ä½¿ç”¨ä¸è¯¥æ–‡ä»¶åŒç›®å½•ä¸‹çš„ edge_browser_data ç›®å½•ä¿å­˜ profile
            user_data_dir = os.path.join(os.path.dirname(__file__), 'edge_browser_data')
            # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆä¼šåˆ›å»ºç›®å½•ï¼‰
            self._ensure_user_data_dir(user_data_dir)
            co.set_user_data_path(user_data_dir)
            print(f"âœ… ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•: {user_data_dir}")

            # ä¸ºäº†æ‰‹åŠ¨ç™»å½•å¹¶ä¿ç•™ä¼šè¯ï¼Œå¿…é¡»ä»¥å¯è§æ¨¡å¼è¿è¡Œï¼ˆheadless ä¸‹æ— æ³•äº¤äº’å¼ç™»å½•æˆ–æµè§ˆå™¨å¯èƒ½ä½¿ç”¨ä¸´æ—¶profileï¼‰
            if self.headless:
                print("âš ï¸ use_saved_login å·²å¯ç”¨ï¼Œå¼ºåˆ¶å…³é—­ headless æ¨¡å¼ä»¥ä¿ç•™ç™»å½•ä¿¡æ¯ï¼ˆéœ€è¦æ‰‹åŠ¨ç™»å½•ï¼‰")
                self.headless = False

        # æ˜¯å¦æ— å¤´æ¨¡å¼
        if self.headless:
            co.headless()
        else:
            co.headless(False)
            co.set_argument('--start-maximized')

        try:
            # åˆ›å»ºé¡µé¢
            self.page = ChromiumPage(addr_or_opts=co)

            # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
            try:
                self.page.run_js('''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                ''')
            except Exception:
                pass

            print("âœ… Edgeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

            # å¦‚æœå¯ç”¨äº†ä¿å­˜ç™»å½•ï¼Œåˆ™æ£€æµ‹æ˜¯å¦å·²ç™»å½•ï¼›è‹¥æœªç™»å½•ï¼Œæç¤ºæ‰‹åŠ¨ç™»å½•å¹¶ç­‰å¾…
            if self.use_saved_login:
                # small delay to allow profile to initialize
                time.sleep(1)
                try:
                    self._ensure_logged_in_or_prompt()
                except Exception as e:
                    print(f"âš ï¸ ç™»å½•æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")

        except Exception as e:
            print(f"âŒ Edgeæµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            exit()

    def _ensure_user_data_dir(self, path: str):
        """ç¡®ä¿ç”¨æˆ·æ•°æ®ç›®å½•å­˜åœ¨å¹¶å¯å†™"""
        try:
            if not os.path.exists(path):
                os.makedirs(path, exist_ok=True)
            # åœ¨ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªå ä½æ–‡ä»¶ï¼Œç¡®ä¿ç›®å½•æ˜¯å¯å†™çš„
            test_file = os.path.join(path, '.profile_write_test')
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write('ok')
            try:
                os.remove(test_file)
            except Exception:
                pass
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆ›å»ºç”¨æˆ·æ•°æ®ç›®å½• {path}: {e}")

    def _ensure_logged_in_or_prompt(self, timeout: int = 180):
        """
        æ£€æŸ¥å½“å‰ profile æ˜¯å¦å·²ç™»å½• Amazonã€‚\
        è‹¥æœªç™»å½•åˆ™ä¿æŒæµè§ˆå™¨å¯è§ï¼Œæç¤ºç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼Œç­‰å¾…ç”¨æˆ·å®Œæˆæˆ–è‡ªåŠ¨æ£€æµ‹åˆ°å·²ç™»å½•åç»§ç»­ã€‚
        """
        try:
            print("ğŸ” æ£€æŸ¥æ˜¯å¦å·²ç™»å½• Amazon...")
            # æ‰“å¼€é¦–é¡µä»¥è¯»å–è´¦å·ä¿¡æ¯
            self.page.get(self.base_url)
            time.sleep(2)

            start = time.time()
            prompted = False
            while time.time() - start < timeout:
                # å°è¯•è·å–é¡¶éƒ¨è´¦å·å…ƒç´ æ–‡æœ¬
                acct_elem = None
                try:
                    acct_elem = self.page.ele('#nav-link-accountList-nav-line-1') or self.page.ele('#nav-link-accountList')
                except Exception:
                    acct_elem = None

                text = ''
                try:
                    if acct_elem and acct_elem.text:
                        text = acct_elem.text.strip().lower()
                except Exception:
                    text = ''

                # å¦‚æœæ–‡æœ¬ä¸­ä¸åŒ…å« sign in åˆ™è®¤ä¸ºå¯èƒ½å·²ç™»å½•ï¼ˆç®€å•åˆ¤æ–­ï¼‰
                if acct_elem and text and 'sign' not in text:
                    print(f"âœ… å·²æ£€æµ‹åˆ°ç™»å½•ï¼š{acct_elem.text.strip()}")
                    return

                # æœªç™»å½•
                if not prompted:
                    print("âš ï¸ æœªæ£€æµ‹åˆ°å·²ç™»å½•è´¦æˆ·ã€‚è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½• Amazonã€‚")
                    print("ç™»å½•å®Œæˆåï¼Œç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹æˆ–æŒ‰ Enter è·³è¿‡ç­‰å¾…ã€‚")
                    prompted = True

                # æ¯éš”ä¸€æ®µæ—¶é—´æ£€æŸ¥ä¸€æ¬¡
                for _ in range(6):
                    time.sleep(2)
                    try:
                        acct_elem = self.page.ele('#nav-link-accountList-nav-line-1') or self.page.ele('#nav-link-accountList')
                        if acct_elem and acct_elem.text and 'sign' not in acct_elem.text.strip().lower():
                            print(f"âœ… å·²æ£€æµ‹åˆ°ç™»å½•ï¼š{acct_elem.text.strip()}")
                            return
                    except Exception:
                        pass

                # æç¤ºç”¨æˆ·å¯ç«‹å³å®Œæˆå¹¶æŒ‰ Enter ç»§ç»­ï¼ˆé¿å…æ— é™ç­‰å¾…ï¼‰
                try:
                    input("å¦‚æœå·²å®Œæˆç™»å½•ï¼Œè¯·æŒ‰ Enter ç»§ç»­ï¼ˆæˆ–ç­‰å¾…è‡ªåŠ¨æ£€æµ‹ï¼‰...")
                except Exception:
                    # åœ¨æŸäº›åœºæ™¯ input å¯èƒ½ä¸å¯ç”¨ï¼Œç»§ç»­æ£€æµ‹ç›´åˆ°è¶…æ—¶
                    pass

            print("âš ï¸ ç™»å½•æ£€æµ‹è¶…æ—¶ï¼Œç»§ç»­è¿è¡Œï¼ˆåç»­å¯èƒ½ä¼šé‡åˆ°éªŒè¯æˆ–éœ€è¦ç™»å½•ï¼‰ã€‚")
        except Exception as e:
            print(f"âš ï¸ ç™»å½•æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

    def search_and_crawl(self, keyword: str, max_products: int = 10, max_pages: int = 1) -> List[Dict]:
        """
        æœç´¢å…³é”®è¯å¹¶çˆ¬å–å•†å“è¯¦æƒ…

        Args:
            keyword: æœç´¢å…³é”®è¯
            max_products: æœ€å¤§çˆ¬å–å•†å“æ•°é‡
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°

        Returns:
            å•†å“è¯¦æƒ…åˆ—è¡¨
        """
        all_products = []

        try:
            print(f"\nğŸ” å¼€å§‹æœç´¢: {keyword}")
            print(f"è®¡åˆ’çˆ¬å–: æœ€å¤š {max_products} ä¸ªå•†å“ï¼Œ{max_pages} é¡µ")

            # 1. æ‰“å¼€äºšé©¬é€Šå¹¶æœç´¢
            self._open_amazon_and_search(keyword)

            # 2. é€é¡µçˆ¬å–
            for page_num in range(1, max_pages + 1):
                print(f"\n{'='*60}")
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ")
                print(f"{'='*60}")

                # ç­‰å¾…æœç´¢ç»“æœåŠ è½½
                self._wait_for_search_results()

                # è·å–å½“å‰é¡µé¢çš„å•†å“é“¾æ¥
                product_links = self._get_product_links_from_page(max_products - len(all_products))

                if not product_links:
                    print("âš ï¸ æœ¬é¡µæ²¡æœ‰æ‰¾åˆ°å•†å“")
                    break

                print(f"æœ¬é¡µæ‰¾åˆ° {len(product_links)} ä¸ªå•†å“ï¼Œå¼€å§‹çˆ¬å–è¯¦æƒ…...")

                # 3. çˆ¬å–æ¯ä¸ªå•†å“çš„è¯¦æƒ…
                for idx, (title, url) in enumerate(product_links, 1):
                    print(f"\n[{idx}/{len(product_links)}] çˆ¬å–å•†å“: {title[:50]}...")

                    product_data = self._crawl_product_detail(url, len(all_products) + idx)
                    if product_data:
                        all_products.append(product_data)

                    # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢çˆ¬å–
                    if len(all_products) >= max_products:
                        print(f"å·²è¾¾åˆ°æœ€å¤§çˆ¬å–æ•°é‡ {max_products}")
                        break

                    # é¿å…è¯·æ±‚è¿‡å¿«
                    if idx < len(product_links):
                        time.sleep(2)

                # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåœæ­¢ç¿»é¡µ
                if len(all_products) >= max_products:
                    break

                # å°è¯•ç¿»åˆ°ä¸‹ä¸€é¡µï¼ˆå¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼‰
                if page_num < max_pages:
                    if not self._go_to_next_page():
                        print("æ²¡æœ‰ä¸‹ä¸€é¡µäº†ï¼Œåœæ­¢çˆ¬å–")
                        break
                    time.sleep(2)  # ç­‰å¾…ä¸‹ä¸€é¡µåŠ è½½

            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {len(all_products)} ä¸ªå•†å“è¯¦æƒ…")
            return all_products

        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return all_products

    def _open_amazon_and_search(self, keyword: str):
        """æ‰“å¼€äºšé©¬é€Šå¹¶æ‰§è¡Œæœç´¢"""
        try:
            # æ‰“å¼€äºšé©¬é€Šé¦–é¡µ
            print(f"æ­£åœ¨æ‰“å¼€ {self.base_url} ...")
            self.page.get(self.base_url)
            time.sleep(3)

            # æ‰§è¡Œæœç´¢
            print(f"æœç´¢å…³é”®è¯: {keyword}")

            # æŸ¥æ‰¾æœç´¢æ¡†
            search_box = self.page.ele('#twotabsearchtextbox', timeout=10)
            if not search_box:
                raise Exception("æ‰¾ä¸åˆ°æœç´¢æ¡†")

            # æ¸…ç©ºå¹¶è¾“å…¥å…³é”®è¯
            search_box.clear()
            search_box.input(keyword)
            time.sleep(1)

            # æŸ¥æ‰¾æœç´¢æŒ‰é’®å¹¶ç‚¹å‡»
            search_btn = self.page.ele('#nav-search-submit-button', timeout=5)
            if search_btn:
                search_btn.click()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŒ‰é’®ï¼ŒæŒ‰å›è½¦é”®
                search_box.input('\n')

            print("âœ… æœç´¢è¯·æ±‚å·²æäº¤")
            time.sleep(3)  # ç­‰å¾…æœç´¢ç»“æœåŠ è½½

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            raise

    def _wait_for_search_results(self, timeout: int = 10):
        """ç­‰å¾…æœç´¢ç»“æœåŠ è½½"""
        print("ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")

        start_time = time.time()
        while time.time() - start_time < timeout:
            # å°è¯•å¤šç§æœç´¢ç»“æœé€‰æ‹©å™¨
            result_selectors = [
                'xpath://div[@data-component-type="s-search-result"]',
                'xpath://div[@role="listitem"][@data-asin]',
                'css:div.s-result-item[data-asin]',
                'xpath://div[contains(@class, "s-result-item")]'
            ]

            for selector in result_selectors:
                elements = self.page.eles(selector)
                if elements and len(elements) > 0:
                    print(f"âœ… æ‰¾åˆ° {len(elements)} ä¸ªæœç´¢ç»“æœ")
                    return

            time.sleep(1)

        print("âš ï¸ è¶…æ—¶æœªæ‰¾åˆ°æœç´¢ç»“æœ")

    def _get_product_links_from_page(self, max_links: int) -> List[tuple]:
        """ä»å½“å‰é¡µé¢è·å–å•†å“é“¾æ¥å’Œæ ‡é¢˜"""
        product_links = []

        try:
            # æŸ¥æ‰¾æ‰€æœ‰å•†å“å…ƒç´ 
            result_selectors = [
                'xpath://div[@data-component-type="s-search-result"]',
                'xpath://div[@role="listitem"][@data-asin]',
                'css:div.s-result-item[data-asin]'
            ]

            product_elements = None
            for selector in result_selectors:
                elements = self.page.eles(selector)
                if elements:
                    product_elements = elements
                    break

            if not product_elements:
                return product_links

            # æå–å•†å“é“¾æ¥å’Œæ ‡é¢˜
            for element in product_elements:
                if len(product_links) >= max_links:
                    break

                try:
                    # æå–å•†å“æ ‡é¢˜
                    title_elem = element.ele('xpath:.//h2//span')
                    if title_elem:
                        title = title_elem.text.strip()
                        if not title:
                            continue
                    else:
                        continue

                    # æå–å•†å“é“¾æ¥
                    link_elem = element.ele('xpath:.//a[contains(@href, "/dp/")]')
                    if not link_elem:
                        link_elem = element.ele('xpath:.//a[contains(@href, "/gp/")]')

                    if link_elem:
                        href = link_elem.attr('href') or ''
                        if href:
                            # ç¡®ä¿æ˜¯å®Œæ•´URL
                            if href.startswith('/'):
                                url = urljoin(self.base_url, href)
                            elif href.startswith('http'):
                                url = href
                            else:
                                url = urljoin(self.base_url, '/' + href.lstrip('/'))

                            # æ·»åŠ åˆ°åˆ—è¡¨
                            product_links.append((title, url))

                except Exception as e:
                    print(f"æå–å•†å“é“¾æ¥å¤±è´¥: {e}")
                    continue

            return product_links

        except Exception as e:
            print(f"è·å–å•†å“é“¾æ¥å¤±è´¥: {e}")
            return product_links

    def _crawl_product_detail(self, url: str, index: int) -> Optional[Dict]:
        """çˆ¬å–å•ä¸ªå•†å“è¯¦æƒ…"""
        product_data = {
            'index': index,
            'url': url,
            'title': None,
            'bullet_points': [],
            'price': None,
            'product_details': {},
            'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }

        try:
            print(f"  æ­£åœ¨è®¿é—®å•†å“é¡µé¢...")
            self.page.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½

            # æ£€æŸ¥æ˜¯å¦404æˆ–æ‰¾ä¸åˆ°é¡µé¢
            page_text = self.page.html.lower()
            if 'page not found' in page_text or 'we couldn\'t find that page' in page_text:
                print(f"  âš ï¸ é¡µé¢ä¸å­˜åœ¨æˆ–å·²è¢«ç§»é™¤")
                return product_data

            # æå–æ ‡é¢˜
            product_data['title'] = self._extract_title()

            # æå–äº”ç‚¹æè¿°
            product_data['bullet_points'] = self._extract_bullet_points()

            # æå–ä»·æ ¼
            product_data['price'] = self._extract_price()

            # æå–å•†å“å±æ€§
            product_data['product_details'] = self._extract_product_details()

            print(f"  âœ… å•†å“è¯¦æƒ…çˆ¬å–æˆåŠŸ")
            return product_data

        except Exception as e:
            print(f"  âŒ çˆ¬å–å•†å“è¯¦æƒ…å¤±è´¥: {e}")
            return product_data

    def _extract_title(self) -> Optional[str]:
        """æå–å•†å“æ ‡é¢˜"""
        try:
            # å°è¯•å¤šä¸ªæ ‡é¢˜é€‰æ‹©å™¨
            title_selectors = [
                'xpath://span[@id="productTitle"]',
                'xpath://h1[@id="title"]//span',
                'xpath://h1[contains(@class, "product-title")]',
            ]

            for selector in title_selectors:
                title_element = self.page.ele(selector, timeout=3)
                if title_element and title_element.text:
                    title = title_element.text.strip()
                    print(f"    æ ‡é¢˜: {title[:60]}...")
                    return title
        except:
            pass

        print("    âš ï¸ æå–æ ‡é¢˜å¤±è´¥")
        return None

    def _extract_bullet_points(self) -> List[str]:
        """æå–äº”ç‚¹æè¿°"""
        bullet_points = []
        try:
            # å°è¯•å¤šä¸ªäº”ç‚¹æè¿°é€‰æ‹©å™¨
            bullet_selectors = [
                'xpath://div[@id="feature-bullets"]',
                'xpath://div[@id="detailBullets_feature_div"]',
                'xpath://ul[contains(@class, "a-unordered-list") and contains(@class, "a-vertical")]',
            ]

            for selector in bullet_selectors:
                bullets_container = self.page.ele(selector, timeout=3)
                if bullets_container:
                    # æŸ¥æ‰¾æ‰€æœ‰ li å…ƒç´ 
                    li_elements = bullets_container.eles('tag:li')
                    for li in li_elements:
                        text = li.text.strip()
                        # è¿‡æ»¤æ‰ç©ºæ–‡æœ¬å’Œæ— å…³å†…å®¹
                        if text and len(text) > 5 and 'see more' not in text.lower():
                            bullet_points.append(text)
                    break

            print(f"    äº”ç‚¹æè¿°: å…± {len(bullet_points)} æ¡")
        except Exception as e:
            print(f"    âš ï¸ æå–äº”ç‚¹æè¿°å¤±è´¥: {e}")
        return bullet_points

    def _extract_price(self) -> Optional[str]:
        """æå–ä»·æ ¼ä¿¡æ¯"""
        try:
            # ä»·æ ¼é€‰æ‹©å™¨
            price_selectors = [
                'xpath://span[@class="a-price"]//span[@class="a-offscreen"]',
                'xpath://span[contains(@class, "a-price-whole")]',
                'xpath://span[contains(@class, "a-price")]//span[@aria-hidden="true"]'
            ]

            # è´§å¸ç¬¦å·
            currency_symbols = ['$', 'Â¥', 'â‚¬', 'Â£']

            for selector in price_selectors:
                price_elements = self.page.eles(selector)
                if price_elements:
                    for price_element in price_elements:
                        price_text = price_element.text.strip()
                        if price_text and any(symbol in price_text for symbol in currency_symbols):
                            print(f"    ä»·æ ¼: {price_text}")
                            return price_text

            print("    âš ï¸ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
            return None

        except Exception as e:
            print(f"    âš ï¸ æå–ä»·æ ¼å¤±è´¥: {e}")
            return None

    def _extract_product_details(self) -> Dict[str, str]:
        """æå–å•†å“å±æ€§è¡¨"""
        details = {}

        try:
            # æŸ¥æ‰¾è¯¦æƒ…è¡¨
            detail_selectors = [
                'xpath://table[@id="productDetails_techSpec_section_1"]',
                'xpath://table[@id="productDetails_detailBullets_sections1"]',
                'xpath://table[contains(@class, "prodDetTable")]',
            ]

            for selector in detail_selectors:
                table = self.page.ele(selector, timeout=3)
                if table:
                    rows = table.eles('tag:tr')
                    for row in rows:
                        try:
                            th = row.ele('tag:th')
                            td = row.ele('tag:td')

                            if th and td:
                                key = th.text.strip().rstrip(':')
                                value = td.text.strip()

                                if key and value:
                                    details[key] = value
                        except:
                            continue
                    break

            print(f"    å•†å“å±æ€§: å…± {len(details)} é¡¹")

        except Exception as e:
            print(f"    âš ï¸ æå–å•†å“è¯¦æƒ…å¤±è´¥: {e}")

        return details

    def _go_to_next_page(self) -> bool:
        """ç¿»åˆ°ä¸‹ä¸€é¡µ"""
        try:
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            next_btn = self.page.ele('css:a.s-pagination-next', timeout=5)

            if next_btn and 's-pagination-disabled' not in (next_btn.attr('class') or ''):
                next_btn.click()
                print("âœ… å·²ç¿»é¡µï¼Œç­‰å¾…æ–°é¡µé¢åŠ è½½...")
                time.sleep(3)  # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                return True
            else:
                print("âš ï¸ ä¸‹ä¸€é¡µæŒ‰é’®ä¸å¯ç”¨æˆ–å·²ç¦ç”¨")
                return False

        except Exception as e:
            print(f"âŒ ç¿»é¡µå¤±è´¥: {e}")
            return False

    def save_results(self, products: List[Dict], filename: str = None):
        """
        ä¿å­˜çˆ¬å–ç»“æœ

        Args:
            products: å•†å“åˆ—è¡¨
            filename: ä¿å­˜æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        """
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"amazon_search_details_{timestamp}.json"

        try:
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            data_to_save = {
                'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_products': len(products),
                'products': products
            }

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            output_path = Path(__file__).parent / filename
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ä¸ºJSON
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONç»“æœå·²ä¿å­˜åˆ°: {output_path}")

            # åŒæ—¶ä¿å­˜ä¸ºCSVä¾¿äºæŸ¥çœ‹
            csv_filename = str(output_path).replace('.json', '.csv')
            self._save_to_csv(products, csv_filename)

        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def _save_to_csv(self, products: List[Dict], filename: str):
        """ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
        try:
            # å®šä¹‰CSVåˆ—
            fieldnames = [
                'index', 'title', 'price', 'url',
                'bullet_points', 'product_details'
            ]

            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                for product in products:
                    # å¤„ç†bullet_pointsåˆ—è¡¨
                    bullet_points_str = ''
                    if 'bullet_points' in product and isinstance(product['bullet_points'], list):
                        bullet_points_str = ' | '.join(product['bullet_points'])

                    # å¤„ç†product_detailså­—å…¸
                    details_str = ''
                    if 'product_details' in product and isinstance(product['product_details'], dict):
                        details_str = ' | '.join(f'{k}: {v}' for k, v in product['product_details'].items())

                    # åªå†™å…¥éœ€è¦çš„åˆ—
                    row = {
                        'index': product.get('index', ''),
                        'title': product.get('title', '')[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        'price': product.get('price', ''),
                        'url': product.get('url', ''),
                        'bullet_points': bullet_points_str[:500],  # é™åˆ¶é•¿åº¦
                        'product_details': details_str[:500]  # é™åˆ¶é•¿åº¦
                    }
                    writer.writerow(row)

            print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ—¶å‡ºé”™: {e}")

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            try:
                self.page.quit()
                print("âœ… Edgeæµè§ˆå™¨å·²å…³é—­")
            except:
                pass


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Amazon å•†å“æœç´¢è¯¦æƒ…ä¸€ä½“åŒ–çˆ¬è™«")
    print("=" * 60)
    print("è¯´æ˜ï¼šè¾“å…¥å…³é”®è¯ç›´æ¥æœç´¢å¹¶çˆ¬å–å•†å“è¯¦æƒ…")
    print("=" * 60)

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        print("\næ­£åœ¨åˆå§‹åŒ–çˆ¬è™«...")
        crawler = AmazonSearchDetailCrawler(
            headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            use_saved_login=True
        )

        # è¾“å…¥æœç´¢å…³é”®è¯
        print("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯:")
        keyword = input("å…³é”®è¯: ").strip()

        if not keyword:
            keyword = "laptop"  # é»˜è®¤å…³é”®è¯
            print(f"ä½¿ç”¨é»˜è®¤å…³é”®è¯: {keyword}")

        # è¾“å…¥æœ€å¤§å•†å“æ•°é‡
        print("\nè¯·è¾“å…¥æœ€å¤§çˆ¬å–å•†å“æ•°é‡ (å»ºè®®10-20):")
        try:
            max_products = int(input("æ•°é‡: ").strip() or "10")
            if max_products < 1:
                max_products = 10
        except:
            max_products = 10
            print(f"ä½¿ç”¨é»˜è®¤æ•°é‡: {max_products}")

        # è¾“å…¥æœ€å¤§é¡µæ•°
        print("\nè¯·è¾“å…¥æœ€å¤§çˆ¬å–é¡µæ•° (å»ºè®®1-2):")
        try:
            max_pages = int(input("é¡µæ•°: ").strip() or "1")
            if max_pages < 1:
                max_pages = 1
        except:
            max_pages = 1
            print(f"ä½¿ç”¨é»˜è®¤é¡µæ•°: {max_pages}")

        # å¼€å§‹çˆ¬å–
        print(f"\nå¼€å§‹æœç´¢å¹¶çˆ¬å–: '{keyword}' ...")
        print("è¯·ç­‰å¾…ï¼Œä¸è¦æ“ä½œæµè§ˆå™¨çª—å£...")

        products = crawler.search_and_crawl(
            keyword=keyword,
            max_products=max_products,
            max_pages=max_pages
        )

        # ä¿å­˜ç»“æœ
        if products:
            # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
            safe_keyword = re.sub(r'[^\w\s-]', '', keyword)[:20]
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"amazon_{safe_keyword}_{timestamp}.json"

            crawler.save_results(products, filename)

            # æ˜¾ç¤ºæ‘˜è¦ä¿¡æ¯
            print(f"\nğŸ“Š çˆ¬å–æ‘˜è¦:")
            print(f"   å…³é”®è¯: {keyword}")
            print(f"   å•†å“æ•°é‡: {len(products)}")
            print(f"   è¾“å‡ºæ–‡ä»¶: {filename}")
            print(f"   åŒæ—¶ç”Ÿæˆ: {filename.replace('.json', '.csv')}")

            # æ˜¾ç¤ºå‰å‡ ä¸ªå•†å“çš„ä¿¡æ¯
            if products:
                print(f"\nğŸ“¦ å‰3ä¸ªå•†å“ä¿¡æ¯:")
                for i, product in enumerate(products[:3], 1):
                    title = product.get('title', 'æ— æ ‡é¢˜')[:50]
                    price = product.get('price', 'N/A')
                    bullets_count = len(product.get('bullet_points', []))
                    details_count = len(product.get('product_details', {}))
                    print(f"   {i}. {title}")
                    print(f"      ä»·æ ¼: {price} | äº”ç‚¹æè¿°: {bullets_count}æ¡ | å±æ€§: {details_count}é¡¹")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å•†å“")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ç¡®ä¿å…³é—­æµè§ˆå™¨ï¼ˆä»…åœ¨ crawler å·²æˆåŠŸåˆ›å»ºæ—¶è°ƒç”¨ï¼‰
        try:
            if 'crawler' in locals() and locals().get('crawler'):
                locals().get('crawler').close()
        except Exception:
            pass

    print("\n" + "=" * 60)
    print("ç¨‹åºæ‰§è¡Œå®Œæ¯•")
    print("=" * 60)
    input("æŒ‰ Enter é”®é€€å‡º...")


if __name__ == '__main__':
    main()