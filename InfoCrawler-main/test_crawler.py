#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Amazon çˆ¬è™«æµ‹è¯•è„šæœ¬ - ä½¿ç”¨ Microsoft Edge æµè§ˆå™¨
"""

import os
import sys
from amazon_search_crawler import AmazonSearchCrawler
import json


def test_search():
    """æµ‹è¯•æœç´¢åŠŸèƒ½ - ä½¿ç”¨ Microsoft Edge æµè§ˆå™¨"""
    print("=" * 60)
    print("Amazon å•†å“æœç´¢æµ‹è¯•")
    print("æµè§ˆå™¨: Microsoft Edge")
    print("æ¨¡å¼: è‡ªåŠ¨å¯åŠ¨")
    print("=" * 60)

    try:
        # ========== æ£€æŸ¥ Edge æµè§ˆå™¨ ==========
        print("\nğŸ” æ£€æŸ¥ Microsoft Edge æµè§ˆå™¨...")

        edge_paths = [
            r'C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe',
            r'C:\Program Files\Microsoft\Edge\Application\msedge.exe',
            r'C:\Users\{}\AppData\Local\Microsoft\Edge\Application\msedge.exe'.format(os.getlogin()),
        ]

        edge_found = False
        for path in edge_paths:
            if os.path.exists(path):
                print(f"âœ… æ‰¾åˆ° Microsoft Edge: {path}")
                edge_found = True
                break

        if not edge_found:
            print("âš ï¸ æœªæ‰¾åˆ° Microsoft Edgeï¼Œå°†å°è¯•ç³»ç»Ÿé»˜è®¤æµè§ˆå™¨")

        # åˆ›å»ºçˆ¬è™«å®ä¾‹ - ä½¿ç”¨æ¥ç®¡æ¨¡å¼ï¼ˆç«¯å£9333ï¼‰
        print("\nğŸš€ å°è¯•æ¥ç®¡ Edge æµè§ˆå™¨ï¼ˆç«¯å£: 9333ï¼‰...")
        print("ğŸ’¡ å¦‚æœæ¥ç®¡å¤±è´¥ï¼Œå°†è‡ªåŠ¨å¯åŠ¨æ–°æµè§ˆå™¨")

        try:
            # å…ˆå°è¯•æ¥ç®¡æ¨¡å¼ï¼ˆå¦‚æœEdgeå·²ç”¨è°ƒè¯•æ¨¡å¼å¯åŠ¨ï¼‰
            crawler = AmazonSearchCrawler(
                headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼Œä¾¿äºè§‚å¯Ÿ
                local_port=9333  # æ¥ç®¡æ¨¡å¼ç«¯å£
            )
            print("âœ… æˆåŠŸæ¥ç®¡ Edge æµè§ˆå™¨")
        except Exception as e:
            print(f"âŒ æ¥ç®¡å¤±è´¥: {e}")
            print("\nğŸ”„ æ­£åœ¨è‡ªåŠ¨å¯åŠ¨ Edge æµè§ˆå™¨...")

            # æ¥ç®¡å¤±è´¥ï¼Œä½¿ç”¨è‡ªåŠ¨å¯åŠ¨æ¨¡å¼
            crawler = AmazonSearchCrawler(
                headless=False,  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
                browser_type='edge'  # æŒ‡å®šä½¿ç”¨Edgeæµè§ˆå™¨
            )
            print("âœ… Edge æµè§ˆå™¨å·²è‡ªåŠ¨å¯åŠ¨")

        # æµ‹è¯•æœç´¢å…³é”®è¯å’Œæœ€å¤§é¡µæ•°
        keyword = "airplane"
        max_pages = 2

        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹æµ‹è¯•æœç´¢: {keyword}")
        print(f"æœ€å¤§çˆ¬å–é¡µæ•°: {max_pages}")
        print(f"{'=' * 60}\n")

        # æ‰§è¡Œæœç´¢
        results = crawler.search_products(keyword, max_pages=max_pages)

        # è¾“å‡ºç»“æœ
        print(f"\n{'=' * 60}")
        print(f"æœç´¢å®Œæˆï¼å…±è·å– {len(results)} ä¸ªå•†å“")
        print(f"{'=' * 60}\n")

        if results:
            # æ˜¾ç¤ºå‰3ä¸ªå•†å“çš„è¯¦ç»†ä¿¡æ¯
            for i, product in enumerate(results[:3], 1):
                print(f"å•†å“ {i}:")
                print(f"  æ ‡é¢˜: {product.get('title', 'N/A')}")
                print(f"  ä»·æ ¼: {product.get('price', 'N/A')}")
                print(f"  ASIN: {product.get('asin', 'N/A')}")
                print(f"  è¯„åˆ†: {product.get('rating', 'N/A')}")
                print(f"  è¯„è®ºæ•°: {product.get('review_count', 'N/A')}")
                detail_url = product.get('detail_url', 'N/A')
                print(f"  è¯¦æƒ…é“¾æ¥: {detail_url[:80] + '...' if detail_url and detail_url != 'N/A' else detail_url}")
                image_url = product.get('image_url', 'N/A')
                print(f"  å›¾ç‰‡é“¾æ¥: {image_url[:80] + '...' if image_url and image_url != 'N/A' else image_url}")
                print()

            # ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶
            output_file = f"amazon_results_{keyword.replace(' ', '_')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("âš ï¸ æœªè·å–åˆ°ä»»ä½•å•†å“æ•°æ®")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # å…³é—­æµè§ˆå™¨
        if 'crawler' in locals():
            print("\næ­£åœ¨å…³é—­æµè§ˆå™¨...")
            try:
                crawler.close()
                print("âœ… æµè§ˆå™¨å·²å…³é—­")
            except:
                print("âš ï¸ å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™")


if __name__ == "__main__":
    test_search()