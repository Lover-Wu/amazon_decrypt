import pandas as pd
import numpy as np
import jieba
import re
import warnings
import json
from dotenv import load_dotenv
from openai import OpenAI
import os
from typing import List, Dict, Union

# æœºå™¨å­¦ä¹ /ç®—æ³•ç›¸å…³
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from gensim import corpora, models
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from mlxtend.frequent_patterns import apriori, association_rules
from snownlp import SnowNLP

# å…¨å±€é…ç½®
warnings.filterwarnings('ignore')
load_dotenv()  # åŠ è½½.envæ–‡ä»¶

# ===================== 1. å¯æ‹“å±•é…ç½® =====================
# JSONæ–‡ä»¶è·¯å¾„ï¼ˆä»…éœ€é…ç½®è¿™ä¸ªï¼‰
JSON_FILE_PATH = "amazon_sg_æˆ’æŒ‡_20260129_160359.json"

# å…¨å±€å˜é‡ï¼ˆåŠ¨æ€èµ‹å€¼ï¼‰
CATEGORY_NAME = ""
SIMULATE_FIELDS = {
    "category": "",
    "search_volume": (10000, 100000),
    "review": {}
}


def init_deepseek_client() -> OpenAI:
    """åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯ï¼ˆå¤ç”¨å¹¶ä¼˜åŒ–ï¼‰"""
    api_key = os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        raise ValueError("é”™è¯¯ï¼šæœªæ‰¾åˆ°DEEPSEEK_API_KEYï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")

    client = OpenAI(
        api_key=api_key,
        base_url="https://api.deepseek.com"
    )
    return client


def extract_category_by_ai(client: OpenAI, product_titles: List[str]) -> str:
    """AIè¯†åˆ«å•†å“æ ¸å¿ƒå“ç±»ï¼ˆå¼ºåˆ¶ä»…è¿”å›ä¸€ä¸ªæ ¸å¿ƒå“ç±»ï¼‰"""
    if not product_titles:
        raise ValueError("æ— å•†å“æ ‡é¢˜æ•°æ®ï¼Œæ— æ³•è¯†åˆ«å“ç±»")

    # é‡‡æ ·å‰20ä¸ªæ ‡é¢˜ï¼ˆé¿å…tokenè¿‡å¤šï¼‰
    sample_titles = product_titles[:20]
    # é‡æ„æç¤ºè¯ï¼šå¼ºåŒ–"ä»…è¿”å›ä¸€ä¸ªæ ¸å¿ƒå“ç±»"çš„ä¸¥æ ¼çº¦æŸ
    prompt = f"""
    è¯·ä»ä»¥ä¸‹äºšé©¬é€Šå•†å“æ ‡é¢˜ä¸­è¯†åˆ«**å”¯ä¸€çš„æ ¸å¿ƒå“ç±»åç§°**ï¼Œä¸¥æ ¼éµå®ˆä»¥ä¸‹è¦æ±‚ï¼š
    1. å¿…é¡»ä»…è¿”å›**ä¸€ä¸ª**æ ¸å¿ƒå“ç±»åç§°ï¼Œç»å¯¹ç¦æ­¢è¿”å›å¤šä¸ªå“ç±»
    2. åªè¿”å›å“ç±»åæœ¬èº«ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šã€è¯´æ˜ã€æ ‡ç‚¹ã€åºå·æˆ–å¤šä½™æ–‡å­—
    3. å“ç±»åæ ¼å¼å‚è€ƒï¼šæ— çº¿è€³æœºã€å……ç”µå®ã€æ™ºèƒ½æ‰‹è¡¨ã€ä¿æ¸©æ¯ï¼ˆä»…ä¸­æ–‡ï¼Œ2-6å­—ï¼‰

    å•†å“æ ‡é¢˜åˆ—è¡¨ï¼š
    {sample_titles}
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,  # é™ä½éšæœºæ€§ï¼Œæå‡ç»“æœç¨³å®šæ€§
            max_tokens=50
        )
        category = response.choices[0].message.content.strip()
        # å…œåº•å¤„ç†ï¼šè¿‡æ»¤æ‰€æœ‰éä¸­æ–‡å­—ç¬¦ï¼Œç¡®ä¿ä»…ä¿ç•™æ ¸å¿ƒå“ç±»å
        category = re.sub(r'[^\u4e00-\u9fa5]', '', category).strip()
        # äºŒæ¬¡å…œåº•ï¼šè‹¥ä»ä¸ºç©ºæˆ–è¿‡é•¿ï¼Œé»˜è®¤å–ç¬¬ä¸€ä¸ªåˆç†å“ç±»
        if not category or len(category) > 10:
            category = "æœªè¯†åˆ«å“ç±»"
        print(f"AIè¯†åˆ«æ ¸å¿ƒå“ç±»ï¼š{category}")
        return category
    except Exception as e:
        raise Exception(f"AIè¯†åˆ«å“ç±»å¤±è´¥ï¼š{str(e)}")


def extract_product_keywords_by_ai(client: OpenAI, title: str, category: str) -> List[str]:
    """AIæå–å•ä¸ªå•†å“æ ‡é¢˜çš„ç²¾å‡†å…³é”®è¯ï¼ˆé€‚é…å“ç±»ï¼‰"""
    if not title:
        return []

    prompt = f"""
    è¯·æå–ä»¥ä¸‹{category}å•†å“æ ‡é¢˜çš„æ ¸å¿ƒå…³é”®è¯ï¼ˆä»…è¿”å›ä¸­æ–‡å…³é”®è¯åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼Œæ¯ä¸ªå…³é”®è¯é•¿åº¦2-4å­—ï¼‰ï¼š
    æ ‡é¢˜ï¼š{title}
    è¦æ±‚ï¼š1. å…³é”®è¯éœ€é€‚é…{category}å“ç±»ç‰¹æ€§ 2. æ’é™¤æ— æ„ä¹‰çš„ä»‹è¯/åŠ©è¯ 3. ä¼˜å…ˆæå–åŠŸèƒ½/å±æ€§/å–ç‚¹
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=100
        )
        keywords = [kw.strip() for kw in response.choices[0].message.content.strip().split(",") if kw.strip()]
        return keywords
    except Exception as e:
        print(f"è­¦å‘Šï¼šAIæå–[{title}]å…³é”®è¯å¤±è´¥ï¼Œä½¿ç”¨åˆ†è¯å…œåº•ï¼š{e}")
        # å…œåº•ï¼šä½¿ç”¨jiebaåˆ†è¯
        stop_words = ['çš„', 'é€‚ç”¨', 'ä¸“ç”¨', 'æ¬¾', 'å‹', 'ä¸º', 'äº†', 'é€‚ç”¨äº', 'for', 'with']
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', title)
        words = jieba.lcut(text)
        return [w for w in words if w not in stop_words and len(w) > 1]


def extract_batch_product_keywords_by_ai(
        client: OpenAI,
        titles: List[str],
        category: str,
        batch_size: int = 50  # æ¯æ‰¹å¤„ç†çš„æ ‡é¢˜æ•°ï¼ˆå¯æ ¹æ®AI Tokené™åˆ¶è°ƒæ•´ï¼‰
) -> List[List[str]]:
    """
    æ‰¹é‡æå–å¤§é‡åŒå“ç±»æ ‡é¢˜çš„å…³é”®è¯ï¼ˆä¼˜åŒ–æ•ˆç‡+æˆæœ¬ï¼‰
    :param client: OpenAIå®¢æˆ·ç«¯å®ä¾‹
    :param titles: å¾…å¤„ç†çš„æ ‡é¢˜åˆ—è¡¨ï¼ˆå¤§é‡ï¼‰
    :param category: ç»Ÿä¸€çš„å“ç±»åç§°
    :param batch_size: æ¯æ‰¹è°ƒç”¨AIçš„æ ‡é¢˜æ•°ï¼ˆé»˜è®¤50ï¼‰
    :return: ä¸æ ‡é¢˜åˆ—è¡¨ä¸€ä¸€å¯¹åº”çš„å…³é”®è¯åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯å•ä¸ªæ ‡é¢˜çš„å…³é”®è¯åˆ—è¡¨ï¼‰
    """
    # æœ€ç»ˆç»“æœåˆ—è¡¨ï¼ˆä¸è¾“å…¥titlesä¸€ä¸€å¯¹åº”ï¼‰
    all_keywords = []

    # æ­¥éª¤1ï¼šè¿‡æ»¤ç©ºæ ‡é¢˜ï¼Œè®°å½•ç©ºæ ‡é¢˜ä½ç½®ï¼ˆä¿è¯ç»“æœä¸è¾“å…¥é¡ºåºä¸€è‡´ï¼‰
    valid_titles = []
    empty_title_indices = []
    for idx, title in enumerate(titles):
        if not title:
            empty_title_indices.append(idx)
            all_keywords.append([])  # ç©ºæ ‡é¢˜ç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        else:
            valid_titles.append((idx, title))  # ä¿ç•™æœ‰æ•ˆæ ‡é¢˜çš„åŸç´¢å¼•å’Œå†…å®¹

    if not valid_titles:
        return all_keywords  # å…¨æ˜¯ç©ºæ ‡é¢˜ï¼Œç›´æ¥è¿”å›

    # æ­¥éª¤2ï¼šåˆ†æ‰¹æ¬¡å¤„ç†æœ‰æ•ˆæ ‡é¢˜ï¼ˆé¿å…å•æ¬¡Tokenè¿‡å¤šï¼‰
    for i in range(0, len(valid_titles), batch_size):
        batch = valid_titles[i:i + batch_size]
        batch_indices = [item[0] for item in batch]  # è¿™æ‰¹æ ‡é¢˜çš„åŸç´¢å¼•
        batch_titles = [item[1] for item in batch]  # è¿™æ‰¹æ ‡é¢˜çš„å†…å®¹

        # æ„å»ºæ‰¹é‡å¤„ç†çš„Promptï¼ˆæ ¸å¿ƒï¼šè®©AIè¿”å›ç»“æ„åŒ–ç»“æœï¼Œæ–¹ä¾¿æ‹†åˆ†ï¼‰
        prompt = f"""
        è¯·æ‰¹é‡æå–ä»¥ä¸‹{category}å“ç±»å•†å“æ ‡é¢˜çš„æ ¸å¿ƒå…³é”®è¯ï¼Œä¸¥æ ¼éµå®ˆè¦æ±‚ï¼š
        1. æ¯ä¸ªæ ‡é¢˜çš„å…³é”®è¯ä»…è¿”å›ä¸­æ–‡ï¼Œç”¨é€—å·åˆ†éš”ï¼Œæ¯ä¸ªå…³é”®è¯é•¿åº¦2-4å­—
        2. å…³é”®è¯éœ€é€‚é…{category}å“ç±»ç‰¹æ€§ï¼Œæ’é™¤æ— æ„ä¹‰çš„ä»‹è¯/åŠ©è¯ï¼Œä¼˜å…ˆæå–åŠŸèƒ½/å±æ€§/å–ç‚¹
        3. æŒ‰æ ‡é¢˜é¡ºåºè¿”å›ï¼Œæ¯ä¸ªæ ‡é¢˜çš„å…³é”®è¯ç”¨ã€===ã€‘åˆ†éš”ï¼Œä»…è¿”å›å…³é”®è¯ï¼Œæ— å…¶ä»–å¤šä½™æ–‡å­—

        å•†å“æ ‡é¢˜åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰ï¼š
        {chr(10).join([f"{idx + 1}. {title}" for idx, title in enumerate(batch_titles)])}
        """

        try:
            # æ‰¹é‡è°ƒç”¨AIï¼ˆä¸€æ¬¡è¯·æ±‚å¤„ç†ä¸€æ‰¹æ ‡é¢˜ï¼Œå¤§å¹…å‡å°‘è°ƒç”¨æ¬¡æ•°ï¼‰
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=2000  # æ‰¹é‡å¤„ç†éœ€å¢å¤§Tokené™åˆ¶ï¼ˆæ ¹æ®batch_sizeè°ƒæ•´ï¼‰
            )

            # è§£æAIè¿”å›çš„æ‰¹é‡ç»“æœ
            ai_result = response.choices[0].message.content.strip()
            # æŒ‰ã€===ã€‘æ‹†åˆ†æ¯ä¸ªæ ‡é¢˜çš„å…³é”®è¯ï¼ˆä¸æ‰¹é‡æ ‡é¢˜é¡ºåºä¸€è‡´ï¼‰
            batch_keywords_str = [kw_str.strip() for kw_str in ai_result.split("===") if kw_str.strip()]

            # å¤„ç†AIè¿”å›ç»“æœï¼ˆä¿è¯æ•°é‡åŒ¹é…ï¼‰
            for j, idx in enumerate(batch_indices):
                if j < len(batch_keywords_str):
                    # æ¸…æ´—å•ä¸ªæ ‡é¢˜çš„å…³é”®è¯
                    keywords = [kw.strip() for kw in batch_keywords_str[j].split(",") if kw.strip()]
                else:
                    # AIè¿”å›ç»“æœæ•°é‡ä¸åŒ¹é…ï¼Œå…œåº•è°ƒç”¨å•æ ‡é¢˜å‡½æ•°
                    keywords = extract_product_keywords_by_ai(client, batch_titles[j], category)
                # å°†ç»“æœæ”¾å…¥åŸç´¢å¼•ä½ç½®
                all_keywords.insert(idx, keywords)

        except Exception as e:
            # æ‰¹é‡AIè°ƒç”¨å¤±è´¥ï¼Œå¯¹è¿™æ‰¹æ ‡é¢˜é€ä¸ªèµ°åŸå•æ ‡é¢˜é€»è¾‘ï¼ˆAI+åˆ†è¯å…œåº•ï¼‰
            print(f"è­¦å‘Šï¼šæ‰¹é‡AIè°ƒç”¨å¤±è´¥ï¼ˆæ‰¹æ¬¡{i // batch_size + 1}ï¼‰ï¼Œé™çº§ä¸ºé€ä¸ªå¤„ç†ï¼š{e}")
            for j, idx in enumerate(batch_indices):
                keywords = extract_product_keywords_by_ai(client, batch_titles[j], category)
                all_keywords.insert(idx, keywords)

    return all_keywords


def batch_title_keywords_extractor(
        input_titles: List[str],
        batch_size: int = 50,
        save_result: bool = True,
        save_path: str = "./batch_keywords_result.json"
) -> Dict[str, List[str]]:
    """
    ç‹¬ç«‹å…¥å£ï¼šè¾“å…¥æ‰¹é‡æ ‡é¢˜ï¼Œç›´æ¥ç”Ÿæˆå¯¹åº”å…³é”®è¯ï¼ˆæ— éœ€ä¾èµ–JSONæ–‡ä»¶ï¼‰
    :param input_titles: æ‰¹é‡å•†å“æ ‡é¢˜åˆ—è¡¨
    :param batch_size: æ¯æ‰¹å¤„ç†çš„æ ‡é¢˜æ•°
    :param save_result: æ˜¯å¦ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    :param save_path: ç»“æœä¿å­˜è·¯å¾„
    :return: å­—å…¸{æ ‡é¢˜: å…³é”®è¯åˆ—è¡¨}
    """
    print("===== å¼€å§‹æ‰¹é‡æ ‡é¢˜å…³é”®è¯æå– =====")
    # 1. åˆå§‹åŒ–AIå®¢æˆ·ç«¯
    try:
        client = init_deepseek_client()
        print(" DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f" å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return {}

    # 2. AIè¯†åˆ«æ ¸å¿ƒå“ç±»
    try:
        category = extract_category_by_ai(client, input_titles)
        print(f"âœ… æ ¸å¿ƒå“ç±»è¯†åˆ«å®Œæˆï¼š{category}")
    except Exception as e:
        print(f"å“ç±»è¯†åˆ«å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨é»˜è®¤å“ç±»'æœªè¯†åˆ«å“ç±»'")
        category = "æœªè¯†åˆ«å“ç±»"

    # 3. æ‰¹é‡æå–å…³é”®è¯
    try:
        all_keywords = extract_batch_product_keywords_by_ai(client, input_titles, category, batch_size)
        print(f" æ‰¹é‡å…³é”®è¯æå–å®Œæˆï¼Œå…±å¤„ç†{len(input_titles)}ä¸ªæ ‡é¢˜")
    except Exception as e:
        print(f" æ‰¹é‡æå–å¤±è´¥ï¼š{e}ï¼Œé™çº§ä¸ºé€ä¸ªæå–")
        all_keywords = [extract_product_keywords_by_ai(client, title, category) for title in input_titles]

    # 4. æ„å»ºç»“æœå­—å…¸ï¼ˆæ ‡é¢˜: å…³é”®è¯ï¼‰
    result_dict = {title: keywords for title, keywords in zip(input_titles, all_keywords)}

    # 5. ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
    if save_result:
        try:
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=4)
            print(f" ç»“æœå·²ä¿å­˜åˆ°ï¼š{save_path}")
        except Exception as e:
            print(f" ç»“æœä¿å­˜å¤±è´¥ï¼š{e}")

    # 6. æ‰“å°éƒ¨åˆ†ç»“æœç¤ºä¾‹
    print("\n===== å…³é”®è¯æå–ç»“æœç¤ºä¾‹ï¼ˆå‰5æ¡ï¼‰ =====")
    sample_count = min(5, len(result_dict))
    for idx, (title, keywords) in enumerate(list(result_dict.items())[:sample_count], 1):
        print(f"{idx}. æ ‡é¢˜ï¼š{title[:50]}..." if len(title) > 50 else f"{idx}. æ ‡é¢˜ï¼š{title}")
        print(f"   å…³é”®è¯ï¼š{keywords}")
        print("-" * 80)

    return result_dict


def generate_dynamic_review_template(client: OpenAI, category: str) -> Dict[str, str]:
    """AIç”Ÿæˆé€‚é…å“ç±»çš„è¯„ä»·æ¨¡æ¿ï¼ˆä¼˜åŒ–JSONè§£æå®¹é”™ï¼‰"""
    # å¼ºåŒ–æç¤ºè¯ï¼šå¼ºåˆ¶ä»…è¿”å›çº¯JSONï¼Œæ— ä»»ä½•å¤šä½™æ–‡å­—
    prompt = f"""
    è¯·ä¸º{category}å“ç±»ç”Ÿæˆ6æ¡è¯„ä»·ï¼ˆ3æ¡æ­£é¢ï¼Œ3æ¡è´Ÿé¢ï¼‰ï¼Œä¸¥æ ¼éµå®ˆä»¥ä¸‹è¦æ±‚ï¼š
    1. ä»…è¿”å›JSONæ ¼å¼å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–æ–‡å­—ã€è¯´æ˜ã€æ³¨é‡Šã€æ ‡ç‚¹æˆ–æ¢è¡Œ
    2. JSONæ ¼å¼ç¤ºä¾‹ï¼š{{"å–ç‚¹1": "è¯„ä»·å†…å®¹1", "å–ç‚¹2": "è¯„ä»·å†…å®¹2"}}
    3. è¯„ä»·éœ€è´´åˆ{category}çš„æ ¸å¿ƒå–ç‚¹/ç—›ç‚¹ï¼Œå†…å®¹ç®€æ´çœŸå®ï¼ˆæ¯æ¡10-20å­—ï¼‰
       ### å‚è€ƒç¤ºä¾‹ï¼ˆæ— çº¿è€³æœºå“ç±»ï¼‰ ###
        "éŸ³è´¨æ¸…æ™°": "éŸ³è´¨å¾ˆæ¸…æ™°ï¼Œæ— æ‚éŸ³ï¼Œä½©æˆ´èˆ’é€‚",
        "ç»­èˆªæŒä¹…": "å……ä¸€æ¬¡ç”µå¯ç”¨ä¸€æ•´å¤©ï¼Œç»­èˆªè¶…é¢„æœŸ",
        "è¿æ¥ç¨³å®š": "è“ç‰™è¿æ¥å¿«ï¼Œä¸ä¼šæ–­è¿ï¼Œæ€§ä»·æ¯”é«˜",
        "éŸ³è´¨å·®": "éŸ³è´¨æœ‰æ‚éŸ³ï¼Œä½éŸ³æ•ˆæœå®Œå…¨ä¸è¡Œ",
        "ç»­èˆªçŸ­": "ç»­èˆªåªæœ‰2å°æ—¶ï¼Œé¢‘ç¹å……ç”µå¾ˆéº»çƒ¦",
        "è¿æ¥å¡é¡¿": "è“ç‰™ç»å¸¸æ–­è¿ï¼Œä½¿ç”¨ä½“éªŒå¾ˆå·®"
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,  # é™ä½éšæœºæ€§ï¼Œæå‡æ ¼å¼åˆè§„ç‡
            max_tokens=300,
            response_format={"type": "json_object"}  # å¼ºåˆ¶JSONæ ¼å¼è¿”å›
        )
        ai_content = response.choices[0].message.content.strip()

        # å…³é”®ä¼˜åŒ–ï¼šæå–JSONå†…å®¹ï¼ˆå³ä½¿AIè¿”å›å¤šä½™æ–‡å­—ï¼Œä¹Ÿèƒ½ç²¾å‡†æå–å¤§æ‹¬å·å†…çš„JSONï¼‰
        json_match = re.search(r'\{[\s\S]*\}', ai_content, re.DOTALL)
        if json_match:
            ai_content = json_match.group()  # åªä¿ç•™å¤§æ‹¬å·åŒ…è£¹çš„JSONéƒ¨åˆ†
        else:
            raise ValueError("æœªæå–åˆ°æœ‰æ•ˆJSONå†…å®¹")

        # è§£æJSON
        review_dict = json.loads(ai_content)
        # æ ¡éªŒJSONæ ¼å¼ï¼ˆç¡®ä¿æ˜¯{k:v}ç»“æ„ï¼Œä¸”æ•°é‡ç¬¦åˆè¦æ±‚ï¼‰
        if not isinstance(review_dict, dict) or len(review_dict) != 6:
            raise ValueError(
                f"JSONæ ¼å¼ä¸ç¬¦åˆè¦æ±‚ï¼ˆéœ€6æ¡è¯„ä»·ï¼‰ï¼Œå½“å‰é•¿åº¦ï¼š{len(review_dict) if isinstance(review_dict, dict) else 'éå­—å…¸'}")

        print(f"AIç”Ÿæˆ{category}è¯„ä»·æ¨¡æ¿ï¼š{review_dict}")
        return review_dict
    except Exception as e:
        print(f"è­¦å‘Šï¼šAIç”Ÿæˆè¯„ä»·æ¨¡æ¿å¤±è´¥ï¼ˆ{e}ï¼‰")
        # å…œåº•é»˜è®¤æ¨¡æ¿ï¼ˆé€‚é…æ‰‹æœºå“ç±»ï¼‰
        return {
            "æ€§ä»·æ¯”é«˜": f"{category}ä»·æ ¼å®æƒ ï¼Œä½¿ç”¨ä½“éªŒå¥½",
            "ç»­èˆªé•¿": f"{category}ç»­èˆªæ—¶é—´ä¹…ï¼Œæ»¡è¶³æ—¥å¸¸ä½¿ç”¨",
            "è´¨é‡å¥½": f"{category}åšå·¥ç²¾ç»†ï¼Œè€ç”¨æ€§å¼º",
            "ç»­èˆªçŸ­": f"{category}ç»­èˆªæ‹‰èƒ¯ï¼Œé¢‘ç¹å……ç”µ",
            "è´¨é‡å·®": f"{category}åšå·¥ç²—ç³™ï¼Œå®¹æ˜“æŸå",
            "ä»·æ ¼é«˜": f"{category}ä»·æ ¼åé«˜ï¼Œæ€§ä»·æ¯”ä½"
        }


# ===================== 2. JSONæ•°æ®è¯»å–ä¸ç©ºå€¼å¤„ç† =====================
def read_amazon_json(file_path: str) -> Dict:
    """è¯»å–äºšé©¬é€ŠJSONæ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶è¯»å–å¼‚å¸¸ + é€‚é…æ— total_productsçš„JSONç»“æ„"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            json_str = f.read().strip()
            # ä¿®å¤JSONæ ¼å¼ä¸å®Œæ•´çš„é—®é¢˜ï¼ˆå…œåº•ï¼‰
            if not json_str.endswith("}"):
                # æ›´å®‰å…¨çš„JSONè¡¥å…¨é€»è¾‘
                if json_str.count("{") > json_str.count("}"):
                    json_str += "}" * (json_str.count("{") - json_str.count("}"))
            data = json.loads(json_str)

        # é€‚é…JSONç»“æ„ï¼šçº¯å•†å“æ•°ç»„/å•ä¸ªå•†å“å¯¹è±¡/æ ‡å‡†ç»“æ„
        if isinstance(data, list):
            return {
                "total_products": len(data),
                "products": data
            }
        elif isinstance(data, dict) and "asin" in data and "title" in data and "total_products" not in data:
            return {
                "total_products": 1,
                "products": [data]
            }
        else:
            return data
    except FileNotFoundError:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°JSONæ–‡ä»¶ï¼š{file_path}")
    except json.JSONDecodeError as e:
        raise ValueError(f"JSONè§£æå¤±è´¥ï¼š{e}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§")


def clean_empty_fields(raw_data: Dict) -> List[Dict]:
    """æ¸…æ´—å•†å“æ•°æ®ï¼šè¿‡æ»¤ç©ºå€¼å­—æ®µ"""
    cleaned_products = []
    for product in raw_data.get("products", []):
        if not isinstance(product, dict):
            print(f"è­¦å‘Šï¼šè·³è¿‡éå­—å…¸æ ¼å¼å•†å“æ•°æ®ï¼š{product}")
            continue
        cleaned_product = {
            k: v for k, v in product.items()
            if v not in (None, "", [], {})
        }
        cleaned_products.append(cleaned_product)
    print(f"JSONæ•°æ®æ¸…æ´—å®Œæˆï¼šåŸå§‹{raw_data.get('total_products', 0)}æ¡ â†’ æœ‰æ•ˆ{len(cleaned_products)}æ¡")
    return cleaned_products


def extract_numeric_value(text: str) -> Union[float, None]:
    """æå–å­—ç¬¦ä¸²ä¸­çš„æ•°å€¼ï¼ˆæ”¯æŒS$ã€åƒåˆ†ä½é€—å·ï¼‰"""
    if not isinstance(text, str):
        return None
    # ç§»é™¤æ‰€æœ‰è´§å¸ç¬¦å·å’Œåƒåˆ†ä½é€—å·
    cleaned_text = text.replace("$", "").replace("S$", "").replace("Â¥", "").replace("å…ƒ", "").replace(",", "")
    # åŒ¹é…å¸¦å°æ•°çš„æ•°å€¼
    nums = re.findall(r'(\d*[.]?\d+)', cleaned_text)
    if not nums:
        return None
    num_str = nums[-1]
    try:
        return float(num_str)
    except ValueError:
        return None


def json_to_dataframe(cleaned_products: List[Dict], client: OpenAI) -> pd.DataFrame:
    """å°†æ¸…æ´—åçš„JSONæ•°æ®è½¬æ¢ä¸ºDataFrameï¼ˆæ ¸å¿ƒï¼šçœŸå®é”€é‡/è¯„è®ºæ•°æ›¿ä»£ï¼‰"""
    # 1. æå–æ‰€æœ‰æ ‡é¢˜ï¼ŒAIè¯†åˆ«å“ç±»
    all_titles = [product.get("title", "") for product in cleaned_products if product.get("title")]
    global CATEGORY_NAME, SIMULATE_FIELDS
    CATEGORY_NAME = extract_category_by_ai(client, all_titles)
    SIMULATE_FIELDS["category"] = f"{CATEGORY_NAME}>{CATEGORY_NAME}"
    SIMULATE_FIELDS["review"] = generate_dynamic_review_template(client, CATEGORY_NAME)

    # 2. æ„å»ºDataFrameæ•°æ®
    df_data = {
        "asin": [], "title": [], "price": [], "original_price": [],
        "discount_percentage": [], "category": [], "search_volume": [],
        "sales": [], "review": [], "ai_keywords": [],
        "reviews_count": []
    }

    np.random.seed(42)
    product_titles = [product.get("title", "") for product in cleaned_products]
    # æ‰¹é‡æå–å…³é”®è¯
    batch_keywords = extract_batch_product_keywords_by_ai(client, product_titles, CATEGORY_NAME)

    for idx, product in enumerate(cleaned_products):
        # æ ¸å¿ƒå­—æ®µå¡«å……
        df_data["asin"].append(product.get("asin", ""))
        title = product.get("title", "")
        df_data["title"].append(title)

        # ä»·æ ¼æå–ï¼ˆå¼ºåŒ–å®¹é”™ï¼‰
        price = extract_numeric_value(product.get("price", ""))
        df_data["price"].append(price if price is not None else 0.0)

        # åŸä»·/æŠ˜æ‰£å¤„ç†
        original_price = extract_numeric_value(product.get("original_price", ""))
        df_data["original_price"].append(
            original_price if original_price is not None else price if price is not None else 0.0)

        discount = extract_numeric_value(product.get("discount_percentage", ""))
        df_data["discount_percentage"].append(discount if discount is not None else 0.0)

        df_data["category"].append(SIMULATE_FIELDS["category"])

        # æ ¸å¿ƒä¿®æ”¹ï¼šçœŸå®é”€é‡ä¼˜å…ˆï¼Œæ— åˆ™ç”¨è¯„è®ºæ•°æ›¿ä»£
        # ä¼˜å…ˆæå–çœŸå®é”€é‡
        real_sales = extract_numeric_value(product.get("sales", ""))
        if real_sales is None:
            # æ— çœŸå®é”€é‡åˆ™ç”¨è¯„è®ºæ•°æ›¿ä»£
            real_sales = extract_numeric_value(product.get("reviews", ""))
        # å…œåº•ï¼šæ— æ•°æ®åˆ™è®¾ä¸º0
        sales_value = int(real_sales) if real_sales is not None else 0

        # æœç´¢é‡ä»ä¿ç•™æ¨¡æ‹Ÿï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
        sv_min, sv_max = SIMULATE_FIELDS["search_volume"]
        df_data["search_volume"].append(int(np.random.randint(sv_min, sv_max)))
        df_data["sales"].append(sales_value)

        # æ‰¹é‡å…³é”®è¯èµ‹å€¼
        df_data["ai_keywords"].append(batch_keywords[idx])

        # æå–è¯„è®ºæ•°å­—æ®µ
        reviews_count = extract_numeric_value(product.get("reviews", ""))
        df_data["reviews_count"].append(reviews_count if reviews_count is not None else 0)

        # åŸºäºAIå…³é”®è¯åŒ¹é…è¯„ä»·
        review = "ä½¿ç”¨ä½“éªŒè‰¯å¥½"
        title_lower = title.lower()
        for keyword, r in SIMULATE_FIELDS["review"].items():
            if keyword in title_lower or any(kw in title_lower for kw in batch_keywords[idx]):
                review = r
                break
        df_data["review"].append(review)

    # è½¬æ¢ä¸ºDataFrameå¹¶æ¸…æ´—
    df = pd.DataFrame(df_data)
    # è¿‡æ»¤ç©ºä»·æ ¼/ç©ºæ ‡é¢˜æ•°æ®
    df = df[df["price"] > 0]
    df = df.dropna(subset=["title"])
    # ç±»å‹æ ‡å‡†åŒ–
    df["price"] = df["price"].astype(float)
    df["original_price"] = df["original_price"].astype(float)
    df["discount_percentage"] = df["discount_percentage"].astype(float)
    df["search_volume"] = df["search_volume"].astype(int)
    df["sales"] = df["sales"].astype(int)
    df["reviews_count"] = df["reviews_count"].astype(int)
    print(f"DataFrameæ„å»ºå®Œæˆï¼šå…±{len(df)}æ¡æœ‰æ•ˆè®°å½•ï¼ˆè¿‡æ»¤ç©ºä»·æ ¼åï¼‰")
    return df


# ===================== 3. æ•°æ®é¢„å¤„ç†ï¼ˆæ•´åˆAIå…³é”®è¯ï¼‰ =====================
def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ•°æ®é¢„å¤„ç†ï¼šæ•´åˆAIå…³é”®è¯+åŸæœ‰åˆ†è¯"""
    # 1. åˆ†ç±»æ‹†åˆ†
    split_category = df["category"].str.split(">", n=1, expand=True)
    if split_category.shape[1] == 0:
        split_category = pd.DataFrame([""] * len(df), columns=["col1"]).assign(col2="")
    elif split_category.shape[1] == 1:
        split_category = split_category.assign(col2="")
    split_category.columns = ["category_1", "category_2"]
    df = pd.concat([df, split_category], axis=1)
    df["category_3"] = df.apply(
        lambda x: x["category_2"] if x["category_2"] != "" else x["category_1"],
        axis=1
    )

    # 2. å…³é”®è¯æ•´åˆ
    stop_words = ['çš„', 'é€‚ç”¨', 'ä¸“ç”¨', 'æ¬¾', 'å‹', 'ä¸º', 'äº†', 'é€‚ç”¨äº', 'for', 'with']

    def preprocess_title(text, ai_keywords):
        if pd.isna(text):
            return ""
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
        words = jieba.lcut(text)
        words = [w for w in words if w not in stop_words and len(w) > 1]
        all_words = list(set(words + ai_keywords))
        return ' '.join(all_words)

    df["title_cut"] = df.apply(lambda x: preprocess_title(x["title"], x["ai_keywords"]), axis=1)

    # 3. åˆ†ç®±é€»è¾‘ï¼ˆå¼ºåˆ¶åˆå§‹åŒ–åˆ—ï¼Œé¿å…ç¼ºå¤±ï¼‰
    # ä»·æ ¼åˆ†ç®±
    df["price_bin"] = None
    if len(df[df["price"] > 0]) > 0:
        df["price_bin"] = pd.cut(
            df["price"],
            bins=[0, 100, 300, 1000, float('inf')],
            labels=["ä½ä»·(<100å…ƒ)", "ä¸­ä»·(100-300å…ƒ)", "é«˜ä»·(300-1000å…ƒ)", "è¶…é«˜ä»·(>1000å…ƒ)"],
            include_lowest=True,
            right=False
        ).astype(object)
    df["price_bin"] = df["price_bin"].fillna("æœªçŸ¥ä»·æ ¼")

    # é”€é‡åˆ†ç®±
    df["sales_bin"] = None
    df["sales_bin"] = pd.cut(
        df["sales"],
        bins=[0, 1000, 3000, 5000],
        labels=["ä½é”€é‡", "ä¸­é”€é‡", "é«˜é”€é‡"],
        include_lowest=True,
        right=False
    ).astype(object)
    df["sales_bin"] = df["sales_bin"].fillna("æœªçŸ¥é”€é‡")

    # æŠ˜æ‰£åˆ†ç®±
    df["discount_bin"] = None
    df["discount_bin"] = pd.cut(
        df["discount_percentage"],
        bins=[0, 5, 10, float('inf')],
        labels=["æ— æŠ˜æ‰£", "å°æŠ˜æ‰£(5%)", "å¤§æŠ˜æ‰£(>10%)"],
        include_lowest=True,
        right=False
    ).astype(object)
    df["discount_bin"] = df["discount_bin"].fillna("æ— æŠ˜æ‰£")

    return df


# ===================== æ–°å¢åŠŸèƒ½ï¼šæ ‡é¢˜æ¨¡å¼ä¸å…³é”®è¯ç»„åˆ†æ =====================
def title_pattern_analysis(df: pd.DataFrame, client: OpenAI) -> Dict:
    """
    æ ‡é¢˜æ·±åº¦åˆ†æï¼š
    1. å…³é”®è¯ç»„ï¼ˆN-Gramï¼‰ç»Ÿè®¡
    2. è¯ç»„ä½ç½®æƒé‡è®¡ç®—
    3. æ ‡é¢˜è¡Œæ–‡é£æ ¼åˆ†æ
    """
    print("\n=== æ–°å¢ï¼šæ ‡é¢˜æ¨¡å¼ä¸å…³é”®è¯ç»„æ·±åº¦åˆ†æ ===")
    result = {}
    valid_titles = df[df["title"] != ""]["title"].tolist()
    if not valid_titles:
        return {"error": "æ— æœ‰æ•ˆæ ‡é¢˜æ•°æ®"}

    # 1. å…³é”®è¯ç»„ï¼ˆN-Gramï¼‰ç»Ÿè®¡
    vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english')
    ngram_matrix = vectorizer.fit_transform(valid_titles)
    ngram_counts = ngram_matrix.sum(axis=0).A1
    ngram_freq = pd.Series(ngram_counts, index=vectorizer.get_feature_names_out()).sort_values(ascending=False)
    top_20_ngrams = ngram_freq.head(20).to_dict()
    result["å…³é”®è¯ç»„é¢‘ç‡"] = top_20_ngrams
    print("é«˜é¢‘å…³é”®è¯ç»„TOP10ï¼š", list(top_20_ngrams.keys())[:10])

    # 2. è¯ç»„ä½ç½®æƒé‡è®¡ç®—
    position_weights = {}
    for title in valid_titles:
        words = re.sub(r'[^\w\s]', '', title.lower()).split()
        for idx, word in enumerate(words, 1):
            # å•è¯è¯­æƒé‡
            weight = 1.0 / idx
            if word not in position_weights:
                position_weights[word] = 0.0
            position_weights[word] += weight
            # 2è¯ç»„åˆæƒé‡
            if idx < len(words):
                phrase = f"{word} {words[idx]}"
                if phrase not in position_weights:
                    position_weights[phrase] = 0.0
                position_weights[phrase] += weight
    # æŒ‰æƒé‡æ’åº
    position_weights = pd.Series(position_weights).sort_values(ascending=False).head(20).to_dict()
    result["è¯ç»„ä½ç½®æƒé‡"] = position_weights
    print("é«˜ä½ç½®æƒé‡è¯ç»„TOP10ï¼š", list(position_weights.keys())[:10])

    # 3. æ ‡é¢˜è¡Œæ–‡é£æ ¼åˆ†æ
    sample_titles = valid_titles[:10]
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹äºšé©¬é€Š{str(CATEGORY_NAME)}å•†å“æ ‡é¢˜çš„æµè¡Œè¡Œæ–‡é£æ ¼å’Œç»“æ„æ¨¡å¼ï¼Œä¸¥æ ¼éµå®ˆï¼š
    1. æ€»ç»“å¸¸è§çš„å¼€å¤´è¯ã€å–ç‚¹æ’å¸ƒé¡ºåºã€å¸¸ç”¨å¥å¼
    2. ç»™å‡ºå¯ç›´æ¥å¤ç”¨çš„æ ‡é¢˜ç»“æ„æ¨¡æ¿
    3. è¯­è¨€ç®€æ´ï¼Œé€‚åˆè¿è¥äººå‘˜å‚è€ƒ

    æ ‡é¢˜ç¤ºä¾‹ï¼š
    {chr(10).join(sample_titles)}
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=500
        )
        style_analysis = response.choices[0].message.content.strip()
        result["è¡Œæ–‡é£æ ¼åˆ†æ"] = style_analysis
        print("æ ‡é¢˜è¡Œæ–‡é£æ ¼åˆ†æå®Œæˆ")
    except Exception as e:
        print(f" é£æ ¼åˆ†æå¤±è´¥ï¼š{e}")
        result["è¡Œæ–‡é£æ ¼åˆ†æ"] = "æœªè·å–åˆ°é£æ ¼åˆ†æç»“æœ"

    return result


# ===================== 4. å¤šç»´åº¦é‡åŒ–åˆ†æ =====================
def multi_dimension_analysis(df: pd.DataFrame, client: OpenAI) -> Dict:
    """æ ¸å¿ƒé‡åŒ–åˆ†æï¼šå…³é”®è¯+é”€é‡é¢„æµ‹+å…³è”è§„åˆ™+æƒ…æ„Ÿåˆ†æ+æ ‡é¢˜æ¨¡å¼åˆ†æ"""
    analysis_result = {}

    # 4.1 å…³é”®è¯åˆ†æï¼ˆTF-IDF+LDAï¼‰
    print("\n=== 1. å…³é”®è¯ä¸ä¸»é¢˜åˆ†æ ===")
    valid_title_cut = df[df["title_cut"] != ""]["title_cut"]
    keyword_freq = None
    keyword_score_df = None

    if len(valid_title_cut) == 0:
        analysis_result["å…³é”®è¯åˆ†æ"] = {"error": "æ— æœ‰æ•ˆæ ‡é¢˜åˆ†è¯æ•°æ®"}
    else:
        # TF-IDFæå–æ ¸å¿ƒå…³é”®è¯
        tfidf = TfidfVectorizer(max_features=20)
        tfidf_matrix = tfidf.fit_transform(valid_title_cut)
        core_keywords = tfidf.get_feature_names_out().tolist()

        # LDAä¸»é¢˜èšç±»
        texts = [text.split() for text in valid_title_cut]
        dictionary = corpora.Dictionary(texts)
        corpus = [dictionary.doc2bow(text) for text in texts]
        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42, passes=10)
        topics = [f"ä¸»é¢˜{idx + 1}ï¼š{topic}" for idx, topic in lda_model.print_topics(num_words=5)]

        # å…³é”®è¯çƒ­åº¦-ç«äº‰åº¦è¯„åˆ†
        keyword_freq = pd.Series([w for text in texts for w in text]).value_counts()
        top_10_keywords = keyword_freq.head(10).index.tolist()
        keyword_scores = []
        for kw in top_10_keywords:
            heat = df[df["title_cut"].str.contains(kw)]["search_volume"].mean() / 1000
            competition = keyword_freq[kw] / keyword_freq.max()
            score = heat / (competition + 0.1)
            keyword_scores.append({
                "å…³é”®è¯": kw,
                "çƒ­åº¦(å½’ä¸€åŒ–)": float(round(heat, 2)),
                "ç«äº‰åº¦(0-1)": float(round(competition, 2)),
                "æ½œåŠ›è¯„åˆ†": float(round(score, 2))
            })
        keyword_score_df = pd.DataFrame(keyword_scores).sort_values("æ½œåŠ›è¯„åˆ†", ascending=False)

        # ä¿å­˜ç»“æœ
        analysis_result["å…³é”®è¯åˆ†æ"] = {
            "æ ¸å¿ƒå…³é”®è¯": core_keywords,
            "ä¸»é¢˜èšç±»": topics,
            "é«˜æ½œåŠ›å…³é”®è¯": keyword_score_df.to_dict("records"),
            "å…³é”®è¯é¢‘ç‡": {k: int(v) for k, v in keyword_freq.to_dict().items()}
        }
        print("æ ¸å¿ƒå…³é”®è¯ï¼š", core_keywords)
        print("é«˜æ½œåŠ›å…³é”®è¯TOP3ï¼š", keyword_score_df.head(3).to_dict("records"))

    # 4.2 æ ‡é¢˜æ¨¡å¼ä¸å…³é”®è¯ç»„åˆ†æ
    analysis_result["æ ‡é¢˜æ¨¡å¼åˆ†æ"] = title_pattern_analysis(df, client)

    # 4.3 é”€é‡é¢„æµ‹ä¸éœ€æ±‚åˆ†æ
    print("\n=== 2. é”€é‡é¢„æµ‹ä¸éœ€æ±‚åˆ†æ ===")
    valid_features = df.dropna(subset=["price", "search_volume", "sales"])
    future_sales_pred_list = []
    price_sales_corr = 0.0

    if len(valid_features) < 10:
        analysis_result["é”€é‡åˆ†æ"] = {"error": "æœ‰æ•ˆæ•°æ®é‡ä¸è¶³ï¼ˆéœ€è‡³å°‘10æ¡ï¼‰ï¼Œæ— æ³•è¿›è¡Œé”€é‡é¢„æµ‹"}
    else:
        features = ["price", "search_volume", "discount_percentage"]
        X = valid_features[features]
        y = valid_features["sales"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # è®­ç»ƒXGBoostæ¨¡å‹
        xgb_model = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
        xgb_model.fit(X_train, y_train)
        y_pred = xgb_model.predict(X_test)

        # æ¨¡å‹è¯„ä¼°
        mae = float(round(mean_absolute_error(y_test, y_pred), 2))
        r2 = float(round(r2_score(y_test, y_pred), 2))
        price_sales_corr = float(round(valid_features[["price", "sales"]].corr().loc["price", "sales"], 2))

        # æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹
        future_price_bands = pd.DataFrame({
            "price": [99, 199, 299],
            "search_volume": [80000, 60000, 40000],
            "discount_percentage": [5, 10, 15]
        })
        future_sales_pred = xgb_model.predict(future_price_bands)
        future_sales_pred_list = [int(round(x)) for x in future_sales_pred]

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = [float(round(x, 3)) for x in xgb_model.feature_importances_]

        analysis_result["é”€é‡åˆ†æ"] = {
            "æ¨¡å‹è¯„ä¼°": {"MAE": mae, "RÂ²": r2},
            "ä»·æ ¼-é”€é‡ç›¸å…³æ€§": price_sales_corr,
            "æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹": [
                {"ä»·æ ¼": 99, "é¢„æµ‹é”€é‡": future_sales_pred_list[0]},
                {"ä»·æ ¼": 199, "é¢„æµ‹é”€é‡": future_sales_pred_list[1]},
                {"ä»·æ ¼": 299, "é¢„æµ‹é”€é‡": future_sales_pred_list[2]}
            ],
            "ç‰¹å¾é‡è¦æ€§": dict(zip(features, feature_importance))
        }
        print(f"é”€é‡æ¨¡å‹RÂ²ï¼š{r2:.2f}ï¼ˆè¶Šæ¥è¿‘1é¢„æµ‹è¶Šå‡†ï¼‰")
        print(f"ä»·æ ¼-é”€é‡ç›¸å…³æ€§ï¼š{price_sales_corr:.2f}ï¼ˆè´Ÿæ•°è¡¨ç¤ºä»·æ ¼è¶Šé«˜é”€é‡è¶Šä½ï¼‰")

    # 4.4 ä»·æ ¼-é”€é‡-æŠ˜æ‰£å…³è”è§„åˆ™åˆ†æ
    print("\n=== 3. ä»·æ ¼-é”€é‡-æŠ˜æ‰£å…³è”è§„åˆ™åˆ†æ ===")
    assoc_df = pd.get_dummies(df[["price_bin", "sales_bin", "discount_bin"]])

    # é™ä½æœ€å°æ”¯æŒåº¦é˜ˆå€¼ï¼Œé€‚é…åˆ†æ•£æ•°æ®
    frequent_itemsets = apriori(assoc_df, min_support=0.05, use_colnames=True)

    if frequent_itemsets.empty:
        analysis_result["å…³è”è§„åˆ™"] = {"error": "æ— æ»¡è¶³æœ€å°æ”¯æŒåº¦çš„é¢‘ç¹é¡¹é›†ï¼Œæ— æ³•ç”Ÿæˆå…³è”è§„åˆ™"}
        print(" æ— æœ‰æ•ˆé¢‘ç¹é¡¹é›†ï¼Œè·³è¿‡å…³è”è§„åˆ™åˆ†æ")
    else:
        rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)
        core_rules = []
        for idx, row in rules.iterrows():
            antecedents = list(row["antecedents"])
            consequents = list(row["consequents"])
            if any("price_bin" in a or "discount_bin" in a for a in antecedents) and any(
                    "sales_bin" in c for c in consequents):
                core_rules.append({
                    "è§„åˆ™": f"{', '.join(antecedents)} â†’ {', '.join(consequents)}",
                    "æ”¯æŒåº¦": float(round(row["support"], 3)),
                    "ç½®ä¿¡åº¦": float(round(row["confidence"], 3)),
                    "æå‡åº¦": float(round(row["lift"], 3))
                })
        analysis_result["å…³è”è§„åˆ™"] = core_rules if core_rules else {"æç¤º": "æ— æœ‰æ•ˆæ ¸å¿ƒå…³è”è§„åˆ™"}
        print("æ ¸å¿ƒå…³è”è§„åˆ™TOP3ï¼š", core_rules[:3] if core_rules else "æ— ")

    # 4.5 è¯„ä»·æƒ…æ„Ÿåˆ†æ
    print("\n=== 4. è¯„ä»·æƒ…æ„Ÿåˆ†æ ===")

    def get_sentiment(text):
        if pd.isna(text):
            return "ä¸­æ€§"
        s = SnowNLP(text)
        return "æ­£é¢" if s.sentiments > 0.7 else "è´Ÿé¢" if s.sentiments < 0.3 else "ä¸­æ€§"

    df["sentiment"] = df["review"].apply(get_sentiment)
    sentiment_dist = {k: int(v) for k, v in df["sentiment"].value_counts().to_dict().items()}

    # æå–æ­£è´Ÿå‘æ ¸å¿ƒè¯æ±‡
    positive_words = []
    for review in df[df["sentiment"] == "æ­£é¢"]["review"]:
        if pd.notna(review):
            words = jieba.lcut(review)
            positive_words.extend([w for w in words if len(w) > 1 and w not in ['çš„', 'äº†', 'å¾ˆ']])

    negative_words = []
    for review in df[df["sentiment"] == "è´Ÿé¢"]["review"]:
        if pd.notna(review):
            words = jieba.lcut(review)
            negative_words.extend([w for w in words if len(w) > 1 and w not in ['çš„', 'äº†', 'å¾ˆ']])

    analysis_result["æƒ…æ„Ÿåˆ†æ"] = {
        "æƒ…æ„Ÿåˆ†å¸ƒ": sentiment_dist,
        "æ­£é¢æ ¸å¿ƒè¯æ±‡": {k: int(v) for k, v in
                         pd.Series(positive_words).value_counts().head(10).to_dict().items()} if positive_words else {},
        "è´Ÿé¢æ ¸å¿ƒè¯æ±‡": {k: int(v) for k, v in
                         pd.Series(negative_words).value_counts().head(10).to_dict().items()} if negative_words else {}
    }
    print("æƒ…æ„Ÿåˆ†å¸ƒï¼š", sentiment_dist)
    print("ç”¨æˆ·æ ¸å¿ƒç—›ç‚¹ï¼š", list(pd.Series(negative_words).value_counts().head(5).index) if negative_words else [])

    # è¿”å›åˆ†æç»“æœ+è¾…åŠ©æ•°æ®
    return analysis_result, {
        "keyword_freq": keyword_freq,
        "keyword_score_df": keyword_score_df,
        "valid_features": valid_features,
        "price_sales_corr": price_sales_corr,
        "future_price_list": [99, 199, 299],
        "future_sales_pred_list": future_sales_pred_list,
        "sentiment_dist": sentiment_dist,
        "df": df
    }


# ===================== 5. AIç”Ÿæˆå“ç±»å»ºè®® =====================
def generate_category_suggestion(client: OpenAI, analysis_result: Dict, category_name: str = CATEGORY_NAME) -> str:
    """è°ƒç”¨DeepSeek AIç”Ÿæˆå“ç±»è¿è¥å»ºè®®"""
    # æå–å¹¶æ ¡éªŒå„å­—æ®µ
    keyword_analysis = analysis_result.get('å…³é”®è¯åˆ†æ', {})
    high_potential_kws = keyword_analysis.get('é«˜æ½œåŠ›å…³é”®è¯', [])
    if not isinstance(high_potential_kws, list):
        high_potential_kws = []
    top3_high_potential = high_potential_kws[:3]

    core_kws = keyword_analysis.get('æ ¸å¿ƒå…³é”®è¯', []) if isinstance(keyword_analysis.get('æ ¸å¿ƒå…³é”®è¯'), list) else []
    topic_cluster = keyword_analysis.get('ä¸»é¢˜èšç±»', []) if isinstance(keyword_analysis.get('ä¸»é¢˜èšç±»'), list) else []

    # æ ‡é¢˜æ¨¡å¼åˆ†æç»“æœ
    title_pattern = analysis_result.get('æ ‡é¢˜æ¨¡å¼åˆ†æ', {})
    top_ngrams = list(title_pattern.get('å…³é”®è¯ç»„é¢‘ç‡', {}).keys())[:5]
    top_position_phrases = list(title_pattern.get('è¯ç»„ä½ç½®æƒé‡', {}).keys())[:5]
    style_analysis = title_pattern.get('è¡Œæ–‡é£æ ¼åˆ†æ', '')

    sales_analysis = analysis_result.get('é”€é‡åˆ†æ', {})
    price_sales_corr = sales_analysis.get('ä»·æ ¼-é”€é‡ç›¸å…³æ€§', 'æ— ')
    price_forecast = sales_analysis.get('æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹', []) if isinstance(
        sales_analysis.get('æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹'), list) else []
    feature_importance = sales_analysis.get('ç‰¹å¾é‡è¦æ€§', {})

    association_rules = analysis_result.get('å…³è”è§„åˆ™', []) if isinstance(analysis_result.get('å…³è”è§„åˆ™'), list) else []

    sentiment_analysis = analysis_result.get('æƒ…æ„Ÿåˆ†æ', {})
    sentiment_dist = sentiment_analysis.get('æƒ…æ„Ÿåˆ†å¸ƒ', {})
    pain_points = list(sentiment_analysis.get('è´Ÿé¢æ ¸å¿ƒè¯æ±‡', {}).keys())[:5]
    advantages = list(sentiment_analysis.get('æ­£é¢æ ¸å¿ƒè¯æ±‡', {}).keys())[:5]

    # æ„é€ æç¤ºè¯
    prompt = f"""
    ä½ æ˜¯èµ„æ·±çš„äºšé©¬é€Šç”µå•†å“ç±»åˆ†æä¸“å®¶ï¼Œç°åœ¨éœ€è¦åŸºäºä»¥ä¸‹{category_name}å“ç±»çš„é‡åŒ–åˆ†æç»“æœï¼Œè¾“å‡ºä¸€ä»½ç»“æ„åŒ–çš„å“ç±»è¿è¥å»ºè®®ã€‚
    åˆ†æç»“æœï¼š
    1. å…³é”®è¯åˆ†æï¼š
       - æ ¸å¿ƒå…³é”®è¯ï¼š{core_kws}
       - é«˜æ½œåŠ›å…³é”®è¯TOP3ï¼š{top3_high_potential}
       - ä¸»é¢˜èšç±»ï¼š{topic_cluster}
    2. æ ‡é¢˜æ¨¡å¼åˆ†æï¼š
       - é«˜é¢‘å…³é”®è¯ç»„ï¼š{top_ngrams}
       - é«˜ä½ç½®æƒé‡è¯ç»„ï¼š{top_position_phrases}
       - è¡Œæ–‡é£æ ¼ï¼š{style_analysis}
    3. é”€é‡åˆ†æï¼š
       - ä»·æ ¼-é”€é‡ç›¸å…³æ€§ï¼š{price_sales_corr}
       - æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹ï¼š{price_forecast}
       - ç‰¹å¾é‡è¦æ€§ï¼š{feature_importance}
    4. å…³è”è§„åˆ™ï¼š{association_rules[:3]}
    5. æƒ…æ„Ÿåˆ†æï¼š
       - æƒ…æ„Ÿåˆ†å¸ƒï¼š{sentiment_dist}
       - ç”¨æˆ·æ ¸å¿ƒç—›ç‚¹ï¼š{pain_points}
       - ç”¨æˆ·æ ¸å¿ƒä¼˜ç‚¹ï¼š{advantages}

    è¦æ±‚ï¼š
    1. è¾“å‡ºç»“æ„æ¸…æ™°ï¼ŒåŒ…å«ã€é€‰å“å»ºè®®ã€‘ã€å®šä»·å»ºè®®ã€‘ã€è¿è¥å»ºè®®ã€‘ã€äº§å“ä¼˜åŒ–å»ºè®®ã€‘ã€æ ‡é¢˜ä¼˜åŒ–å»ºè®®ã€‘5ä¸ªéƒ¨åˆ†ï¼›
    2. å»ºè®®è¦å…·ä½“ã€å¯è½åœ°ï¼Œç»“åˆäºšé©¬é€Šå¹³å°ç‰¹æ€§å’Œæ•°æ®åˆ†æç»“æœï¼Œä¸è¦ç©ºæ³›ï¼›
    3. è¯­è¨€ç®€æ´ï¼Œé€‚åˆäºšé©¬é€Šè¿è¥äººå‘˜é˜…è¯»ï¼›
    4. é‡ç‚¹çªå‡ºé«˜æ½œåŠ›æ–¹å‘å’Œéœ€è¦è§„é¿çš„é£é™©ç‚¹ï¼›
    5. ã€æ ‡é¢˜ä¼˜åŒ–å»ºè®®ã€‘éœ€ç»“åˆé«˜é¢‘å…³é”®è¯ç»„å’Œä½ç½®æƒé‡ç»™å‡ºå…·ä½“ä¼˜åŒ–æ–¹æ¡ˆã€‚
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=5000
        )
        return response.choices[0].message.content
    except Exception as e:
        raise Exception(f"è°ƒç”¨DeepSeek AIå¤±è´¥ï¼š{str(e)}")


# ===================== AIç”Ÿæˆäºšé©¬é€Šçˆ†æ¬¾é•¿æ ‡é¢˜ =====================
def generate_hot_product_titles(client: OpenAI, category_name: str, suggestion: str) -> List[str]:
    """åŸºäºå“ç±»è¿è¥å»ºè®®ç”Ÿæˆ3-5ä¸ªäºšé©¬é€Šçˆ†æ¬¾é•¿æ ‡é¢˜"""
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹{category_name}å“ç±»çš„è¿è¥å»ºè®®ï¼Œä¸ºäºšé©¬é€Šå¹³å°ç”Ÿæˆ3-5ä¸ªæ˜“å¤§å–çš„çˆ†æ¬¾å•†å“æ ‡é¢˜ï¼Œä¸¥æ ¼éµå®ˆæ‰€æœ‰è§„åˆ™ï¼š

    ## æ ¸å¿ƒè¿è¥å»ºè®®
    {suggestion}

    ## æ ‡é¢˜åˆ›ä½œè§„åˆ™
    1. æ ‡é¢˜æ•°é‡ï¼šå¿…é¡»æ˜¯3-5ä¸ªï¼Œä¸å¤šä¸å°‘
    2. æ ‡é¢˜é£æ ¼ï¼šåŒ…å«å¤šç»´åº¦å–ç‚¹ï¼ˆæè´¨/åŠŸèƒ½/åœºæ™¯/é£æ ¼/å±æ€§/ç°è´§ç­‰ï¼‰ï¼Œç¬¦åˆäºšé©¬é€Šçˆ†æ¬¾æ ‡é¢˜ç‰¹å¾
    3. æ ‡é¢˜é•¿åº¦ï¼šæ¯ä¸ªæ ‡é¢˜å­—æ•°20-40å­—ï¼ŒåŒ…å«ä¸°å¯Œçš„å–ç‚¹å…³é”®è¯ï¼Œæ— å†—ä½™è™šè¯
    4. è¾“å‡ºæ ¼å¼ï¼šä»…è¿”å›æ ‡é¢˜åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œæ— å…¶ä»–ä»»ä½•è§£é‡Šã€åºå·ã€æ ‡ç‚¹æˆ–å¤šä½™æ–‡å­—
    5. æ ¸å¿ƒè¦æ±‚ï¼š
       - æ ‡é¢˜å¿…é¡»ç´§å¯†è´´åˆè¿è¥å»ºè®®ä¸­çš„é€‰å“ã€å®šä»·ã€æ ‡é¢˜ä¼˜åŒ–ã€äº§å“ä¼˜åŒ–æ–¹å‘
       - ä¼˜å…ˆèå…¥è¿è¥å»ºè®®ä¸­æåˆ°çš„é«˜æ½œåŠ›å…³é”®è¯ã€æœ€ä¼˜ä»·æ ¼å¸¦ã€æ ¸å¿ƒå–ç‚¹
       - è§„é¿è¿è¥å»ºè®®ä¸­æåˆ°çš„é£é™©ç‚¹ï¼ˆå¦‚ç”¨æˆ·ç—›ç‚¹ã€ä½é”€é‡ä»·æ ¼æ®µï¼‰
       - è´´åˆ{category_name}å“ç±»ç‰¹æ€§ï¼Œçªå‡ºæ ¸å¿ƒå–ç‚¹

    å“ç±»ï¼š{category_name}
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500,
        )
        # è§£ææ ‡é¢˜åˆ—è¡¨
        title_content = response.choices[0].message.content.strip()
        titles = [title.strip() for title in title_content.split("\n") if
                  title.strip() and not title.startswith(("1.", "2.", "3.", "4.", "5.", "(", "ï¼ˆ"))]

        # æ ¡éªŒæ ‡é¢˜æ•°é‡
        if len(titles) < 3:
            supplement_titles = [
                f"çˆ†æ¬¾{category_name} é«˜æ€§ä»·æ¯”{category_name} {category_name}ç°è´§é€Ÿå‘å¤šåœºæ™¯é€‚ç”¨",
                f"çƒ­é”€æ¬¾{category_name} è´´åˆè¿è¥å»ºè®®{category_name} æ ¸å¿ƒå–ç‚¹{category_name}ç°è´§ä¾›åº”",
                f"ä¼˜åŒ–æ¬¾{category_name} è§„é¿ç—›ç‚¹{category_name} é«˜æ½œåŠ›å…³é”®è¯{category_name}ä¾¿æºå®ç”¨æ¬¾"
            ]
            titles += supplement_titles[:3 - len(titles)]
        elif len(titles) > 5:
            titles = titles[:5]

        print(f"\n=== AIåŸºäºè¿è¥å»ºè®®ç”Ÿæˆ{category_name}çˆ†æ¬¾é•¿æ ‡é¢˜ï¼ˆ3-5ä¸ªï¼‰===")
        for idx, title in enumerate(titles, 1):
            print(f"{idx}. {title}")

        return titles
    except Exception as e:
        print(f"AIç”Ÿæˆçˆ†æ¬¾æ ‡é¢˜å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨å…œåº•æ ‡é¢˜")
        fallback_titles = [
            f"ä¼˜åŒ–æ¬¾{category_name} é«˜æ½œåŠ›å…³é”®è¯{category_name} ç°è´§çƒ­é”€å¤šåœºæ™¯é€‚ç”¨{category_name}",
            f"çˆ†æ¬¾{category_name} è´´åˆè¿è¥å»ºè®®{category_name} æ ¸å¿ƒå–ç‚¹çªå‡º{category_name}ç°è´§é€Ÿå‘",
            f"çƒ­é”€{category_name} è§„é¿ç”¨æˆ·ç—›ç‚¹{category_name} é«˜æ€§ä»·æ¯”{category_name}ä¾¿æºå®ç”¨æ¬¾"
        ]
        return fallback_titles


def generate_title_recommendation_score(client: OpenAI, titles: list, category_name: str, sales_analysis: dict,
                                        keyword_analysis: dict, title_pattern_analysis: dict) -> list:
    """AIåˆ†æå¹¶è®¡ç®—æ ‡é¢˜çš„æ¨èæŒ‡æ•°ï¼ˆ0-10åˆ†ï¼‰"""
    # æå–æ ¸å¿ƒæ•°æ®ï¼ˆå¢åŠ å®¹é”™ï¼‰
    price_sales_corr = sales_analysis.get('ä»·æ ¼-é”€é‡ç›¸å…³æ€§', 0.0)
    feature_importance = sales_analysis.get('ç‰¹å¾é‡è¦æ€§', {})
    price_forecast = sales_analysis.get('æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹', [])

    high_potential_kws = keyword_analysis.get('é«˜æ½œåŠ›å…³é”®è¯', [])
    if not isinstance(high_potential_kws, list):
        high_potential_kws = []
    valid_high_potential = [kw for kw in high_potential_kws if isinstance(kw, dict) and "å…³é”®è¯" in kw]

    core_kws = keyword_analysis.get('æ ¸å¿ƒå…³é”®è¯', [])
    if not isinstance(core_kws, list):
        core_kws = []

    top_ngrams = list(title_pattern_analysis.get('å…³é”®è¯ç»„é¢‘ç‡', {}).keys())
    top_position_phrases = list(title_pattern_analysis.get('è¯ç»„ä½ç½®æƒé‡', {}).keys())

    # æ„å»ºè¯„åˆ†Prompt
    prompt = f"""
    ä½ æ˜¯èµ„æ·±äºšé©¬é€Šè¿è¥ä¸“å®¶ï¼Œéœ€åŸºäº{category_name}å“ç±»çš„é”€é‡å…³è”æ•°æ®å’Œæ ‡é¢˜æ¨¡å¼åˆ†æï¼Œä¸ºä»¥ä¸‹æ ‡é¢˜è®¡ç®—æ¨èæŒ‡æ•°ï¼ˆ0-10åˆ†ï¼Œä¿ç•™2ä½å°æ•°ï¼‰ï¼š
    ## æ ¸å¿ƒæ•°æ®èƒŒæ™¯
    1. ä»·æ ¼-é”€é‡ç›¸å…³æ€§ï¼š{price_sales_corr}ï¼ˆè´Ÿæ•°=ä»·æ ¼è¶Šé«˜é”€é‡è¶Šä½ï¼Œæ­£æ•°=ä»·æ ¼è¶Šé«˜é”€é‡è¶Šé«˜ï¼‰
    2. é”€é‡å½±å“ç‰¹å¾é‡è¦æ€§ï¼š{feature_importance}ï¼ˆæ•°å€¼è¶Šé«˜å¯¹é”€é‡å½±å“è¶Šå¤§ï¼‰
    3. æœªæ¥ä»·æ ¼å¸¦é”€é‡é¢„æµ‹ï¼š{price_forecast}
    4. é«˜æ½œåŠ›å…³é”®è¯ï¼ˆå¸¦æ½œåŠ›è¯„åˆ†ï¼‰ï¼š{valid_high_potential}
    5. å“ç±»æ ¸å¿ƒå…³é”®è¯ï¼š{core_kws}
    6. é«˜é¢‘å…³é”®è¯ç»„ï¼š{top_ngrams}
    7. é«˜ä½ç½®æƒé‡è¯ç»„ï¼š{top_position_phrases}

    ## è¯„åˆ†è§„åˆ™
    1. æ ‡é¢˜åŒ…å«é«˜æ½œåŠ›å…³é”®è¯/æ ¸å¿ƒå…³é”®è¯/é«˜é¢‘è¯ç»„è¶Šå¤šï¼Œåˆ†æ•°è¶Šé«˜
    2. æ ‡é¢˜åŒ…å«é«˜ä½ç½®æƒé‡è¯ç»„ä¸”æ”¾ç½®åœ¨é å‰ä½ç½®ï¼Œåˆ†æ•°è¶Šé«˜
    3. æ ‡é¢˜å–ç‚¹è´´åˆé«˜æƒé‡é”€é‡ç‰¹å¾ï¼ˆå¦‚ä»·æ ¼ã€æœç´¢é‡ï¼‰ï¼Œåˆ†æ•°è¶Šé«˜
    4. æ ‡é¢˜ç¬¦åˆäºšé©¬é€Šçˆ†æ¬¾ç‰¹å¾ï¼ˆå–ç‚¹ä¸°å¯Œã€é€‚é…å“ç±»ï¼‰ï¼Œåˆ†æ•°è¶Šé«˜
    5. åˆ†æ•°å¿…é¡»åœ¨0-10ä¹‹é—´ï¼Œä¿ç•™2ä½å°æ•°

    ## éœ€è¦è¯„åˆ†çš„æ ‡é¢˜åˆ—è¡¨
    {[f"{i + 1}. {title}" for i, title in enumerate(titles)]}

    ## è¾“å‡ºè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
    ä»…è¿”å›JSONæ•°ç»„ï¼Œæ— ä»»ä½•å…¶ä»–å†…å®¹ï¼æ ¼å¼ç¤ºä¾‹ï¼š
    [{{"title": "æ ‡é¢˜1", "score": 8.50}}, {{"title": "æ ‡é¢˜2", "score": 7.80}}]
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        ai_content = response.choices[0].message.content.strip()
        ai_content = ai_content.replace("\n", "").replace("\t", "").strip()

        # è§£æJSON
        score_list = json.loads(ai_content)
        # æ ¡éªŒæ ¼å¼
        if not isinstance(score_list, list) or len(score_list) != len(titles):
            raise ValueError(f"è¿”å›åˆ—è¡¨é•¿åº¦ä¸åŒ¹é…ï¼ˆé¢„æœŸ{len(titles)}æ¡ï¼‰")

        # æ ¡éªŒå¹¶ä¿®æ­£åˆ†æ•°
        valid_score_list = []
        for idx, item in enumerate(score_list):
            if not isinstance(item, dict) or "title" not in item or "score" not in item:
                valid_score_list.append({"title": titles[idx], "score": 7.00})
            else:
                score = float(item["score"])
                score = max(0.0, min(10.0, score))
                valid_score_list.append({"title": item["title"], "score": round(score, 2)})

        print(f"\n=== {category_name}æ ‡é¢˜æ¨èæŒ‡æ•°è¯„åˆ†ç»“æœ ===")
        for item in valid_score_list:
            print(f"æ ‡é¢˜ï¼š{item['title'][:50]}... | æ¨èæŒ‡æ•°ï¼š{item['score']}/10.00")

        return valid_score_list

    except Exception as e:
        print(f"âš ï¸ æ ‡é¢˜è¯„åˆ†å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨å…œåº•è¯„åˆ†ï¼ˆ7.0åˆ†ï¼‰")
        fallback_score_list = [{"title": title, "score": 7.00} for title in titles]
        return fallback_score_list


def main():
    """ä¸»å‡½æ•°ï¼šæ•´åˆæ‰€æœ‰åˆ†ææµç¨‹"""
    print("===== äºšé©¬é€Šå•†å“æ•°æ®åˆ†æç³»ç»Ÿå¯åŠ¨ =====")
    try:
        # 1. åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
        client = init_deepseek_client()
        print("âœ… æ­¥éª¤1/9ï¼šDeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        # 2. è¯»å–JSONæ•°æ®
        raw_data = read_amazon_json(JSON_FILE_PATH)
        print("âœ… æ­¥éª¤2/9ï¼šJSONæ•°æ®è¯»å–å®Œæˆ")

        # 3. æ¸…æ´—ç©ºå€¼å­—æ®µ
        cleaned_products = clean_empty_fields(raw_data)
        if not cleaned_products:
            raise ValueError("æ¸…æ´—åæ— æœ‰æ•ˆå•†å“æ•°æ®ï¼Œç»ˆæ­¢åˆ†æ")
        print("âœ… æ­¥éª¤3/9ï¼šæ•°æ®æ¸…æ´—å®Œæˆ")

        # 4. è½¬æ¢ä¸ºDataFrameï¼ˆå«çœŸå®é”€é‡/è¯„è®ºæ•°å¤„ç†ï¼‰
        df = json_to_dataframe(cleaned_products, client)
        print("âœ… æ­¥éª¤4/9ï¼šDataFrameæ„å»ºå®Œæˆ")

        # 5. æ•°æ®é¢„å¤„ç†
        df = preprocess_data(df)
        print("âœ… æ­¥éª¤5/9ï¼šæ•°æ®é¢„å¤„ç†å®Œæˆ")

        # 6. å¤šç»´åº¦é‡åŒ–åˆ†æ
        analysis_result, visual_data = multi_dimension_analysis(df, client)
        print("âœ… æ­¥éª¤6/9ï¼šå¤šç»´åº¦é‡åŒ–åˆ†æå®Œæˆ")

        # 7. AIç”Ÿæˆå“ç±»è¿è¥å»ºè®®
        suggestion = generate_category_suggestion(client, analysis_result)
        print("\n=== AIç”Ÿæˆçš„å“ç±»è¿è¥å»ºè®® ===")
        print(suggestion)
        print("âœ… æ­¥éª¤7/9ï¼šå“ç±»è¿è¥å»ºè®®ç”Ÿæˆå®Œæˆ")

        # 8. AIç”Ÿæˆçˆ†æ¬¾é•¿æ ‡é¢˜
        hot_titles = generate_hot_product_titles(client, CATEGORY_NAME, suggestion)
        print("âœ… æ­¥éª¤8/9ï¼šçˆ†æ¬¾æ ‡é¢˜ç”Ÿæˆå®Œæˆ")

        # 9. æ ‡é¢˜æ¨èæŒ‡æ•°è¯„åˆ†
        title_pattern_analysis_result = analysis_result.get("æ ‡é¢˜æ¨¡å¼åˆ†æ", {})
        keyword_analysis_result = analysis_result.get("å…³é”®è¯åˆ†æ", {})
        sales_analysis_result = analysis_result.get("é”€é‡åˆ†æ", {})
        title_scores = generate_title_recommendation_score(
            client, hot_titles, CATEGORY_NAME,
            sales_analysis_result, keyword_analysis_result, title_pattern_analysis_result
        )
        print("âœ… æ­¥éª¤9/9ï¼šæ ‡é¢˜æ¨èæŒ‡æ•°è¯„åˆ†å®Œæˆ")

        print("\n===== äºšé©¬é€Šå•†å“æ•°æ®åˆ†æç³»ç»Ÿæ‰§è¡Œå®Œæ¯• =====")
        # ä¿å­˜æœ€ç»ˆåˆ†æç»“æœ
        final_result = {
            "å“ç±»åç§°": CATEGORY_NAME,
            "åˆ†æç»“æœ": analysis_result,
            "çˆ†æ¬¾æ ‡é¢˜åŠè¯„åˆ†": title_scores,
            "è¿è¥å»ºè®®": suggestion
        }
        save_path = f"{CATEGORY_NAME}_æœ€ç»ˆåˆ†æç»“æœ.json"
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=4)
        print(f"ğŸ“æœ€ç»ˆåˆ†æç»“æœå·²ä¿å­˜è‡³ï¼š{save_path}")

    except Exception as e:
        print(f"\nåˆ†ææµç¨‹æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        raise


# ç¨‹åºå…¥å£
if __name__ == "__main__":
    main()